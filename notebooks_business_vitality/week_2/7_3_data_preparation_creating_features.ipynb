{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"../images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, and Jonathan Morgan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning - Feature Creation\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Before we begin, run the code cell below to initialize the libraries we'll be using in this assignment. We're already familiar with `numpy`, `pandas`, and `psycopg2` from previous tutorials. Here we'll also be using [`scikit-learn`](http://scikit-learn.org) to fit modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"appliedda\"\n",
    "hostname = \"10.10.2.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert team schema name below:\n",
    "myschema = 'ada_18_uchi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Features\n",
    "\n",
    "Our features are our independent variables or predictors. Good features make machine learning systems effective. \n",
    "The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. \n",
    "\n",
    "Machine Learning Algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem. \n",
    "\n",
    "- **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text.\n",
    "\n",
    "Example of feature engineering are: \n",
    "\n",
    "- **Transformations**, such a log, square, and square root.\n",
    "- **Dummy (binary) variables**, also known as *indicator variables*, often done by taking categorical variables\n",
    "(such as city) which do not have a numerical value, and adding them to models as a binary value.\n",
    "- **Discretization**. Several methods require features to be discrete instead of continuous. This is often done \n",
    "by binning, which you can do by equal width. \n",
    "- **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use \n",
    "different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several\n",
    "values into one feature, aggregating over varying windows of time and space. For example, given urban data, \n",
    "we would want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius\n",
    "of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    ">Our preliminary features are the following\n",
    ">- `new_employer` (Binary): 1 if the employer is \"new\" (did not exist before a cutoff date), 0 if they are not. \n",
    ">- `nb_emp` (Aggregation): Total number of employees working for a given employer at a given year and quarter.\n",
    ">- `total_payroll` (Aggregation): Total amount paid out by the employer in earnings during a given year adn quarter. \n",
    ">- `wage_gap` (Aggregation): TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## New vs Old Employers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a first binary feature defining \"old\" and \"new\" firms. Old firms are determined according to age cutoff, with a default value is 5 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=db_name, host = hostname)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a list of IDs that existed 5 years prior to Q1 of 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TEMP TABLE features_prior_ids AS\n",
    "SELECT CONCAT(ein, '-', seinunit, '-', empr_no) AS id\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE year = 2013-5 AND quarter = 1;\n",
    "COMMIT;\n",
    "'''\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then merge this list to our labels: employers from the labels table that do not match any of the prior ID's will be flagged as \"new\" (they did not exist 5 years ago), while the other ones will be flagged as old."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT a.id, CASE WHEN b.id IS NULL THEN 1 ELSE 0 END AS is_new\n",
    "FROM ada_18_uchi.labels_2013q1_2014q1 AS a\n",
    "LEFT JOIN features_prior_ids AS b\n",
    "ON a.id = b.id;\n",
    "'''\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the first rows of data and check how many employers are \"new\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_new'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to facilitate creating this feature for several years of data, we combined all the above steps into a Python function, and added a final step that writes the feature table to the database.\n",
    "\n",
    "In the step-by-step approach above, all SQL queries were entirely hard coded. The Python function however uses parameters so you can easily reuse it. The function's parameters are:\n",
    "- `year`: The year at which we are doing the prediction.\n",
    "- `qtr`: The quarter at which we are doing the prediction.\n",
    "- `delta_t`: The forward-looking window we chose when creating the model's labels. The default value is 1, which means we are prediction at a given time whether an employer will still exist one year later.\n",
    "- `age_cutoff`: The backwards-looking window we use to define \"new\" employers. The default value is set to 5, which means \"new\" employers are defined as existing since less than 5 years.\n",
    "- `schema`: Your team schema, where the label table will be written. The default value is set to `myschema`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `db_name`: Database name. This is the name of the SQL database we are using. The default value is set to `db_name`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `hostname`: Host name. This is the host name for the SQL database we are using. The default value is set to `hostname`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `overwrite`: Whether you want the function to overwrite tables that already exist. Before writing a table, the function will check whether this table exists, and by default will not overwrite existing tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employer_age_features(year, qtr, delta_t=1, age_cutoff=5, schema=myschema, \n",
    "                          db_name=db_name, hostname=hostname, overwrite=False):\n",
    "    \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "        \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'features_isnew_{year}q{qtr}_age{age_cutoff}' \n",
    "    AND table_schema = '{schema}';\n",
    "    '''.format(year=year, qtr=qtr, age_cutoff=age_cutoff, schema=schema))\n",
    "    \n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        print(\"Creating table\")    \n",
    "        \n",
    "        sql = '''\n",
    "        CREATE TEMP TABLE features_isnew_{year}q{qtr} AS\n",
    "        SELECT CONCAT(ein, '-', seinunit, '-', empr_no) AS id\n",
    "        FROM il_des_kcmo.il_qcew_employers\n",
    "        WHERE year = {year}-{age_cutoff} AND quarter = {qtr};\n",
    "        COMMIT;\n",
    "\n",
    "        DROP TABLE IF EXISTS {schema}.features_isnew_{year}q{qtr}_age{age_cutoff};\n",
    "        CREATE TABLE {schema}.features_isnew_{year}q{qtr}_age{age_cutoff} AS\n",
    "        SELECT a.id, CASE WHEN b.id IS NULL THEN 1 ELSE 0 END AS is_new\n",
    "        FROM {schema}.labels_{year}q{qtr}_{year_pdelta}q{qtr} AS a\n",
    "        LEFT JOIN features_isnew_{year}q{qtr} AS b\n",
    "        ON a.id = b.id;\n",
    "        COMMIT;\n",
    "\n",
    "        ALTER TABLE {schema}.features_isnew_{year}q{qtr}_age{age_cutoff} OWNER TO {schema}_admin;\n",
    "        COMMIT;\n",
    "        '''.format(year=year, qtr=qtr, schema=schema, year_pdelta=year+delta_t, age_cutoff=age_cutoff)  \n",
    "        \n",
    "        cursor.execute(sql)\n",
    "        \n",
    "    else:\n",
    "        print(\"Table already exists\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    sql = '''\n",
    "    SELECT * FROM {schema}.features_isnew_{year}q{qtr}_age{age_cutoff}\n",
    "    '''.format(year=year, qtr=qtr, age_cutoff=age_cutoff, schema=schema)\n",
    "    df = pd.read_sql(sql, conn)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isnew_2013q1 = employer_age_features(2013, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isnew_2013q1['is_new'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isnew_2014q1 = employer_age_features(2014, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_isnew_2014q1['is_new'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Wages and Employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two aggregate features for every employer's total number of employees and total payroll. The features will be created using the QCEW Employer table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=db_name, host = hostname)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by querying from the QCEW Employer table the total number of employees and total payroll at a given quarter (Q1 of 2013). \n",
    "\n",
    "Since the number of employees is given on a month-by-month basis, let's sum the number for the three months. While this number will not reflect the number of unique employees hired by that employer over the quarter, it will reflect the number of monthly positions' wages paid out by the employer (with individuals who are employed in all three months of that quarter counted 3 times). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TEMP TABLE features_emprs AS\n",
    "SELECT CONCAT(ein, '-', seinunit, '-', empr_no) AS id, \n",
    "        empl_month1::int+empl_month2::int+empl_month3::int AS total_empl,\n",
    "        total_wages\n",
    "FROM il_des_kcmo.il_qcew_employers\n",
    "WHERE year = 2013 AND quarter = 1;\n",
    "COMMIT;\n",
    "'''\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge this to our list of labels and visualize the first rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT a.id, b.total_empl, b.total_wages\n",
    "FROM ada_18_uchi.labels_2013q1_2014q1 AS a\n",
    "LEFT JOIN features_emprs AS b\n",
    "ON a.id = b.id;\n",
    "'''\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create an \"average monthly earnings\" feature which will be defined as total payroll divided by total employees, and check summary statistics on the 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_wage'] = df['total_wages']/df['total_empl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['total_empl', 'total_wages', 'avg_wage']].describe(percentiles=[0.01,0.05,0.25,0.50,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of some data inconsistencies in total employees and total wages, some average wages could not be calculated (when `total_empl == 0` and `total_wages == 0`) and some have `inf` values (when `total_empl == 0`). These `NULL` and `inf` values will be problematic for the machine learning algorithm.\n",
    "\n",
    "Instead of dropping these observations, let's impute the missing values of employees and earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of imputing missing values based on the rest of the data. Missing values can be imputed to median of the rest of the data, or you can use other characteristics (industry, geography, etc.) to impute an employer's missing value with the median of similar employers.\n",
    "\n",
    "In this preliminary case, let's impute the missing values to the median value of all the data. Let's start with the Total Number of Employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_replace = df[df['total_empl']==0]['total_empl'].values\n",
    "df['total_empl'].replace(vals_to_replace, np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have replaced all 0-values to NaN, let's check the overall median number of total employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_total_empl = df['total_empl'].median()\n",
    "print(median_total_empl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following row replaces all NaN values in the Average Number of Employees column by the median value found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_empl'].fillna(median_total_empl, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this for the total payroll variable, but this time using a shorter syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_wages'] = np.where(df['total_wages']==0, np.NaN, df['total_wages'])\n",
    "df['total_wages'].fillna(df['total_wages'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we altered Total Employees and Total Earnings, let's redefine the \"Average Earnings\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['avg_wage']\n",
    "df['avg_wage'] = df['total_wages']/df['total_empl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check how the summary statistics have changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.01,0.05,0.25,0.50,0.75,0.95,0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more examples of imputing missing values, see the [8_1_inference_imputing_missing_values](8_1_inference_imputing_missing_values.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers \n",
    "\n",
    "Some values of average monthly wage still seem impossible or very unlikely. Some outliers in particular have average wages far exceeding the 99th percentile or well below the 1st percentile. \n",
    "\n",
    "Usually, you would want to perform a \"sanity check\" on these values with someone who knows the data well. **It is never a good idea to drop observations without prior investigation!** Here, however, for the purposes of this exercise, we chose to drop these values.\n",
    "\n",
    "Let's start by flagging outliers, check what share of the rows we would be deleting, and drop the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all rows where the average monthly wage is below 200 or above 50,000\n",
    "outlier_rows = ((df['avg_wage'] < 200)|(df['avg_wage'] > 50000))\n",
    "df[outlier_rows].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What share of rows are outliers?\n",
    "nrows_wages = df.shape[0]\n",
    "nrows_wages_outliers = df[outlier_rows].shape[0]\n",
    "print('Share outlier rows: {}%'.format(float(nrows_wages_outliers)/nrows_wages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get rid of the outlier rows\n",
    "df = df[~outlier_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of Values\n",
    "\n",
    "Certain models will have issue with the distance between features such as number of employees and average wages. Number of employees is typically a number between 1 and 100 while average wages are usually between 1000 and 10,000. In order to circumvent this problem we can scale our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: let's scale average wages:\n",
    "min_avg_wage = df['avg_wage'].min()\n",
    "max_avg_wage = df['avg_wage'].max()\n",
    "df['avg_wage_scl'] = (df['avg_wage']-min_avg_wage)/(max_avg_wage-min_avg_wage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the distributions of the scaled and unscaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['avg_wage', 'avg_wage_scl']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it again for Total Employees and Total Wages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = df['total_empl'].min()\n",
    "max_val = df['total_empl'].max()\n",
    "df['total_empl_scl'] = (df['total_empl']-min_val)/(max_val-min_val)\n",
    "\n",
    "min_val = df['total_wages'].min()\n",
    "max_val = df['total_wages'].max()\n",
    "df['total_wages_scl'] = (df['total_wages']-min_val)/(max_val-min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataframe now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All above steps have been summarized into a Python function below. We also added a final step that writes the feature table to the database.\n",
    "\n",
    "In the step-by-step approach above, all SQL queries were entirely hard coded. The Python function however uses parameters so you can easily reuse it. The function's parameters are:\n",
    "- `year`: The year at which we are doing the prediction.\n",
    "- `qtr`: The quarter at which we are doing the prediction.\n",
    "- `delta_t`: The forward-looking window we chose when creating the model's labels. The default value is 1, which means we are prediction at a given time whether an employer will still exist one year later.\n",
    "- `schema`: Your team schema, where the label table will be written. The default value is set to `myschema`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `db_name`: Database name. This is the name of the SQL database we are using. The default value is set to `db_name`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `hostname`: Host name. This is the host name for the SQL database we are using. The default value is set to `hostname`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `overwrite`: Whether you want the function to overwrite tables that already exist. Before writing a table, the function will check whether this table exists, and by default will not overwrite existing tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employer_wages_empl(year, qtr, delta_t=1, schema=myschema, \n",
    "                        db_name=db_name, hostname=hostname, overwrite=False):\n",
    "    \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'features_wages_empl_{year}q{qtr}' \n",
    "    AND table_schema = '{schema}';\n",
    "    '''.format(year=year, qtr=qtr, schema=schema))\n",
    "\n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        print(\"Creating table\")\n",
    "        \n",
    "        # Create Temp Table\n",
    "        sql = '''\n",
    "        CREATE TEMP TABLE features_emprs AS\n",
    "        SELECT CONCAT(ein, '-', seinunit, '-', empr_no) AS id, \n",
    "                empl_month1::int+empl_month2::int+empl_month3::int AS total_empl,\n",
    "                total_wages\n",
    "        FROM il_des_kcmo.il_qcew_employers\n",
    "        WHERE year = {year} AND quarter = {qtr};\n",
    "        COMMIT;\n",
    "        '''.format(year=year, qtr=qtr, schema=schema)\n",
    "        cursor.execute(sql)\n",
    "        \n",
    "        # Merge onto labels and load into Python\n",
    "        sql = '''\n",
    "        SELECT a.id, b.total_empl, b.total_wages\n",
    "        FROM ada_18_uchi.labels_{year}q{qtr}_{year_pdelta}q{qtr} AS a\n",
    "        LEFT JOIN features_emprs AS b\n",
    "        ON a.id = b.id;\n",
    "        '''.format(year=year, qtr=qtr, year_pdelta=year+delta_t, schema=schema)\n",
    "        df = pd.read_sql(sql, conn)\n",
    "        \n",
    "        # Impute Missing Values\n",
    "        df['total_empl'] = np.where(df['total_empl']<=0, np.NaN, df['total_empl'])\n",
    "        df['total_empl'].fillna(df['total_empl'].median(), inplace=True)\n",
    "        df['total_wages'] = np.where(df['total_wages']<=0, np.NaN, df['total_wages'])\n",
    "        df['total_wages'].fillna(df['total_wages'].median(), inplace=True)\n",
    "        \n",
    "        # Create Feature Average Wage\n",
    "        df['avg_wage'] = df['total_wages']/df['total_empl']\n",
    "        \n",
    "        # Remove Outliers\n",
    "        outlier_rows = ((df['avg_wage'] < 200)|(df['avg_wage'] > 50000))\n",
    "        df = df[~outlier_rows]\n",
    "        \n",
    "        # Scale Values\n",
    "        min_val = df['total_empl'].min()\n",
    "        max_val = df['total_empl'].max()\n",
    "        df['total_empl_scl'] = (df['total_empl']-min_val)/(max_val-min_val)\n",
    "        min_val = df['total_wages'].min()\n",
    "        max_val = df['total_wages'].max()\n",
    "        df['total_wages_scl'] = (df['total_wages']-min_val)/(max_val-min_val)\n",
    "        min_val = df['avg_wage'].min()\n",
    "        max_val = df['avg_wage'].max()\n",
    "        df['avg_wage_scl'] = (df['avg_wage']-min_val)/(max_val-min_val)\n",
    "        \n",
    "        # Write Table to Database\n",
    "        engine = create_engine('postgresql://{}/{}'.format(hostname, db_name))\n",
    "        df.to_sql('features_wages_empl_{year}q{qtr}'.format(year=year, qtr=qtr), \n",
    "                  engine, schema=myschema, index=False, if_exists='replace', )\n",
    "        sql = '''\n",
    "        ALTER TABLE {schema}.features_wages_empl_{year}q{qtr} OWNER TO {schema}_admin;\n",
    "        COMMIT;\n",
    "        '''.format(year=year, qtr=qtr, schema=schema)\n",
    "        cursor.execute(sql)\n",
    "        \n",
    "    else:\n",
    "        print(\"Table already exists\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    sql = '''\n",
    "    SELECT * FROM {schema}.features_wages_empl_{year}q{qtr} \n",
    "    '''.format(year=year, qtr=qtr, schema=schema)\n",
    "    df = pd.read_sql(sql, conn)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wages_empl_2013q1 = employer_wages_empl(2013, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wages_empl_2013q1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wages_empl_2014q1 = employer_wages_empl(2014, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_wages_empl_2014q1.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firm Wage Gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of using microdata is that you have access to individual wages instead of only aggregate levels. Among the possible features that you can create using individual wages are metrics encapsulating the wage gap between the firm's highest and lowest earners. Here, we present two such metric: the range (difference between the min and max) and the interquartile range (difference between the 75th and 25th percentiles). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(database=db_name, host = hostname)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by querying the Wage data at the year and quarter of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TEMP TABLE all_wages AS\n",
    "SELECT CONCAT(ein, '-', seinunit, '-', empr_no) AS id, ssn, wage\n",
    "FROM il_des_kcmo.il_wage\n",
    "WHERE year = 2013 AND quarter = 1;\n",
    "COMMIT;\n",
    "'''\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many observations does the resulting table have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql('select count(*) from all_wages;', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now merge the temp table we created onto the labels table, load that table into Python and visualize the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TEMP TABLE labels_all_wages AS\n",
    "SELECT a.id, b.ssn, b.wage\n",
    "FROM ada_18_uchi.labels_2013q1_2014q1 AS a\n",
    "LEFT JOIN all_wages AS b\n",
    "ON a.id = b.id;\n",
    "COMMIT;\n",
    "'''\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM labels_all_wages LIMIT 10', conn)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a table where the observation level is the individual employee. In order to get wage gap metrics at employer level, we are going to group by Employer ID and keep certain percentiles of the underlying wages. Let's write the SQL query and load that table into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TEMP TABLE wage_pcts AS\n",
    "SELECT id,\n",
    "        COUNT(DISTINCT ssn) AS nb_empl,\n",
    "        MIN(wage) AS min_wage,\n",
    "        PERCENTILE_CONT(0.25) WITHIN GROUP(ORDER BY wage) AS wage_pct_25,\n",
    "        PERCENTILE_CONT(0.50) WITHIN GROUP(ORDER BY wage) AS med_wage,\n",
    "        PERCENTILE_CONT(0.75) WITHIN GROUP(ORDER BY wage) AS wage_pct_75,        \n",
    "        MAX(wage) AS max_wage\n",
    "FROM labels_all_wages\n",
    "GROUP BY id;\n",
    "COMMIT;\n",
    "'''\n",
    "cursor.execute(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM wage_pcts LIMIT 100', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the wage-gap metrics we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *,\n",
    "        max_wage - min_wage AS wage_range,\n",
    "        wage_pct_75 - wage_pct_25 AS wage_intqtr_range\n",
    "FROM wage_pcts;\n",
    "'''\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the several issues we run into:\n",
    "\n",
    "XXXXXXXXXXX\n",
    "\n",
    "Restricting to firms that do not run into the above issues (firms with at least 5 employees, for example) will reduce largely the scope of our analysis. Here, since we want to include these features in the model, we will make this restriction. But as noted before, always think twice before dropping a large number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT *,\n",
    "        max_wage - min_wage AS wage_range,\n",
    "        wage_pct_75 - wage_pct_25 AS wage_intqtr_range\n",
    "FROM wage_pcts\n",
    "WHERE nb_empl >= 5;\n",
    "'''\n",
    "df = pd.read_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All above steps have been summarized into a Python function below. We also added a final step that writes the feature table to the database.\n",
    "\n",
    "In the step-by-step approach above, all SQL queries were entirely hard coded. The Python function however uses parameters so you can easily reuse it. The function's parameters are:\n",
    "- `year`: The year at which we are doing the prediction.\n",
    "- `qtr`: The quarter at which we are doing the prediction.\n",
    "- `delta_t`: The forward-looking window we chose when creating the model's labels. The default value is 1, which means we are prediction at a given time whether an employer will still exist one year later.\n",
    "- `min_size`: The minimum number of employees. As we saw above, wage gap characteristics make little sense for very small firms. The default value is set to 5.  \n",
    "- `schema`: Your team schema, where the label table will be written. The default value is set to `myschema`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `db_name`: Database name. This is the name of the SQL database we are using. The default value is set to `db_name`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `hostname`: Host name. This is the host name for the SQL database we are using. The default value is set to `hostname`, defined in the [Python Setup](#Python-Setup) section of this notebook.\n",
    "- `overwrite`: Whether you want the function to overwrite tables that already exist. Before writing a table, the function will check whether this table exists, and by default will not overwrite existing tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def employer_wage_gap(year, qtr, min_size=5, delta_t=1,\n",
    "                      schema=myschema, db_name=db_name, hostname=hostname, overwrite=False):\n",
    "    \n",
    "    conn = psycopg2.connect(database=db_name, host = hostname) #database connection\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Let's check if the table already exists:\n",
    "    cursor.execute('''\n",
    "    SELECT * FROM information_schema.tables \n",
    "    WHERE table_name = 'features_wage_gap_{year}q{qtr}_size{min_size}' \n",
    "    AND table_schema = '{schema}';\n",
    "    '''.format(year=year, qtr=qtr, min_size=min_size, schema=schema))\n",
    "\n",
    "    # Let's write table if it does not exist (or if overwrite = True)\n",
    "    if not(cursor.rowcount) or overwrite:\n",
    "        print(\"Creating table\")\n",
    "        \n",
    "        # Create Temp Table\n",
    "        sql = '''\n",
    "        CREATE TEMP TABLE all_wages AS\n",
    "        SELECT CONCAT(ein, '-', seinunit, '-', empr_no) AS id, ssn, wage\n",
    "        FROM il_des_kcmo.il_wage\n",
    "        WHERE year = {year} AND quarter = {qtr};\n",
    "        COMMIT;\n",
    "\n",
    "        CREATE TEMP TABLE labels_all_wages AS\n",
    "        SELECT a.id, b.ssn, b.wage\n",
    "        FROM ada_18_uchi.labels_{year}q{qtr}_{year_pdelta}q{qtr} AS a\n",
    "        LEFT JOIN all_wages AS b\n",
    "        ON a.id = b.id;\n",
    "        COMMIT;     \n",
    "\n",
    "        CREATE TEMP TABLE wage_pcts AS\n",
    "        SELECT id,\n",
    "                COUNT(DISTINCT ssn) AS nb_empl,\n",
    "                MIN(wage) AS min_wage,\n",
    "                PERCENTILE_CONT(0.25) WITHIN GROUP(ORDER BY wage) AS wage_pct_25,\n",
    "                PERCENTILE_CONT(0.50) WITHIN GROUP(ORDER BY wage) AS med_wage,\n",
    "                PERCENTILE_CONT(0.75) WITHIN GROUP(ORDER BY wage) AS wage_pct_75,        \n",
    "                MAX(wage) AS max_wage\n",
    "        FROM labels_all_wages\n",
    "        GROUP BY id;\n",
    "        COMMIT;\n",
    "        \n",
    "        DROP TABLE IF EXISTS {schema}.features_wage_gap_{year}q{qtr}_size{min_size};\n",
    "        CREATE TABLE {schema}.features_wage_gap_{year}q{qtr}_size{min_size} AS\n",
    "        SELECT *,\n",
    "                max_wage - min_wage AS wage_range,\n",
    "                wage_pct_75 - wage_pct_25 AS wage_intqtr_range\n",
    "        FROM wage_pcts\n",
    "        WHERE nb_empl >= {min_size};\n",
    "        COMMIT;\n",
    "        \n",
    "        ALTER TABLE {schema}.features_wage_gap_{year}q{qtr}_size{min_size} OWNER TO {schema}_admin;\n",
    "        COMMIT;\n",
    "        '''.format(year=year, qtr=qtr, min_size=min_size, year_pdelta=year+delta_t, schema=schema)\n",
    "        cursor.execute(sql)\n",
    "        \n",
    "    else:\n",
    "        print(\"Table already exists\")\n",
    "    \n",
    "    cursor.close()\n",
    "    \n",
    "    sql = '''\n",
    "    SELECT * FROM {schema}.features_wage_gap_{year}q{qtr}_size{min_size}\n",
    "    '''.format(year=year, qtr=qtr, min_size=min_size, schema=schema)\n",
    "    df = pd.read_sql(sql, conn)  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wage_gap_2013q1 = employer_wage_gap(2013, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wage_gap_2013q1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_wage_gap_2014q1 = employer_wage_gap(2014, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_wage_gap_2014q1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
