{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spatial Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Simple maps](#Simple-maps)\n",
    "2. [Choropleth](#Choropleths)\n",
    "2. [Choropleth of rate](#Choropleth-of-rate)\n",
    "3. [Choropleth multiple years](#Choropleth-multiple-years)\n",
    "\n",
    "[Addendum](#Addendum)\n",
    "1. Overview of [spatial data types](#Spatial-data-types)\n",
    "2. [Datasets](#Datasets) in this Notebook\n",
    "2. [Coordinate Reference Systems (aka projections)](#Coordinate-Reference-Systems)\n",
    "\n",
    "\n",
    "### Reference material\n",
    "**NOTE: much of the below material combines using SQL via Pandas and Geopandas to get the data in the format we want for spatial analyses or visualization in Python, As a reminder, the \"[Spatial Queries in PostGIS](../spatial_notebooks/Spatial-Queries-in-PostGIS.ipynb)\" has more detailed explanations of PostGIS and will help you better understand the underlying spatial SQL in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple maps\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this notebook we will make use of the 2014Q3 subset of IDES establishment point locations created for the \"spatial queries in PostGIS\" notebook linked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data manipulation libraries ##\n",
    "# Pandas for generic manipulation\n",
    "import pandas as pd\n",
    "# GeoPandas for spatial data manipulation\n",
    "import geopandas as gpd\n",
    "\n",
    "# PySAL for spatial statistics\n",
    "import pysal as ps\n",
    "\n",
    "# shapely for specific spatial data tasks (GeoPandas uses Shapely objects)\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "\n",
    "# SQLAlchemy to get some data from the database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# improve control of visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up connection paramaters for the 'appliedda' database\n",
    "host = '10.10.2.10'\n",
    "db = 'appliedda'\n",
    "\n",
    "# create DB connection \n",
    "# (connection string is \"<database-type>://<username>@<host-url>:<port>/<database-name>\")\n",
    "engine = create_engine(\"postgresql://{host_str}/{db_str}\".format(host_str = host, db_str=db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in point locations - the full file has ~455k locations (~208k of which are in Cook County)\n",
    "# so here we are going to grab a sample of 500 from within Cook County\n",
    "query = \"\"\"\n",
    "SELECT e.*\n",
    "FROM il_des_kcmo.il_des_establishment e\n",
    "JOIN tl_2016_us_county c\n",
    "ON ST_Within(e.geom, c.geom)\n",
    "WHERE c.geoid = '17031' AND e.year = 2014 AND e.quarter = 3\n",
    "LIMIT 500;\n",
    "\"\"\"\n",
    "points = gpd.read_postgis(query, engine, geom_col='geom',\n",
    "                          crs={'init': u'epsg:4269'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what info is contained in this file?\n",
    "points.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the first row look like?\n",
    "points.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does a simple map of those locations look like? \n",
    "# Note that you can pass matplotlib keywords to (geo)pandas, like figsize\n",
    "points.plot(figsize=(4,6))\n",
    "plt.annotate('Source: IDES', xy=(0.8,-0.10), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There's some dots. Not super useful, but we can see by the longitude (x axis) and latitude (y axis) they're where we'd expect for Chicago. \n",
    "\n",
    "Let's add Chicago zip codes too to give some context, to do so we'll pull the polygons from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL query\n",
    "sql = \"\"\"\n",
    "SELECT z.* \n",
    "FROM tl_2016_us_zcta510 z\n",
    "JOIN tl_2016_us_county c\n",
    "ON ST_Within(z.geom, c.geom)\n",
    "WHERE c.geoid = '17031';\n",
    "\"\"\"\n",
    "\n",
    "# get data from the database\n",
    "polys = gpd.read_postgis(sql, engine, geom_col='geom', \n",
    "                               index_col='gid', crs={'init': u'epsg:4269'})\n",
    "# note we need to include the \"geom_col\" and \"crs\" parameters\n",
    "# so that geopandas knows which column containes the geographic info\n",
    "# and the correct Coordinate Reference System of the data\n",
    "\n",
    "\n",
    "# see info of new geodataframe\n",
    "polys.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot counties - using all defaults in pandas's plot() function\n",
    "# adds colors essentially randomly based on order of shapes in the dataframe\n",
    "polys.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try putting the point locations on top of counties, \n",
    "# and let's make counties grey\n",
    "\n",
    "# create a map of IL counties colored grey\n",
    "ax = polys.plot(color='grey', figsize=(6,8))\n",
    "# note we assign counties to the object \"ax\" so we can \n",
    "# overlay the points on the same \"matplotlib axis\"\n",
    "\n",
    "# use the same \"ax\" object to plot the prisons on top of the county map, \n",
    "# plus resize the markers and remove their outlines\n",
    "points.plot(ax=ax, markersize=5)#, markeredgewidth=0); \n",
    "plt.annotate('Source: Census TIGERLines & IDES', xy=(0.8,-0.10), xycoords=\"axes fraction\")\n",
    "plt.title('Business locations');\n",
    "# pro tip: adding this semi-colon at the end stops Jupyter \n",
    "# from printing out the \"<matplotlib.axes....>\" line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Choropleths are very useful because they can quickly convey how values compare across the study area. Let's start wtih a simple example of the land area of each county. (Note much of the code below comes from Sergio and Dani's Geovisualization notebook -> source: http://darribas.org/gds_scipy16/ipynb_md/02_geovisualization.html)\n",
    "\n",
    "Let's start with a simple dataset of number businesses by Chicago neighborhood. SQL used to create the \"chi_nhood_estab_2014q3\" table:\n",
    "\n",
    "    SELECT n.neighborhood, count(e.*) tot_biz, \n",
    "        sum(e.total_wages) wages_tot, sum(e.total_jobs) jobs_tot, n.geom \n",
    "    INTO class2.chi_nhood_estab_2014q3\n",
    "    FROM chicago_nhoods AS n\n",
    "    JOIN class2.il_des_subset_2014q3 e\n",
    "    ON ST_Within(e.geom, n.geom)\n",
    "    GROUP BY n.neighborhood, n.geom\n",
    "    ORDER BY n.neighborhood;\n",
    "    \n",
    "    ALTER TABLE class2.chi_nhood_estab_2014q3 ADD PRIMARY KEY (neighborhood);\n",
    "    CREATE INDEX chi_nhood_estab_2014q3_geom_gist ON class2.chi_nhood_estab_2014q3 USING gist (geom);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data \n",
    "qry = \"SELECT * FROM class2.chi_nhood_estab_2014q3\"\n",
    "\n",
    "nhoods = gpd.read_postgis(qry, engine, geom_col='geom', \n",
    "                          crs='+init=epsg:4269')\n",
    "nhoods.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the descriptive stats?\n",
    "nhoods.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll create our matplotlib figure and axis first so we have more control over it later\n",
    "f, ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# we'll pass the geopandas plot() function the column, scheme (calculation method), \n",
    "# number of groups to calculate (k)\n",
    "# colormap to use, linewidge (to make the edges less noticeable), and \n",
    "# the axis object created above\n",
    "nhoods.plot(column='tot_biz', scheme='QUANTILES', k=10, \n",
    "               cmap='plasma', linewidth=0.1, ax=ax)\n",
    "\n",
    "# and this time we'll turn off the coordinates\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, Geopandas only allows using the \"quantiles\" (or any other [scheme supported by PySAL](http://pysal.readthedocs.io/en/latest/library/esda/mapclassify.html)) with between 2 and 9 and if you try soemthing different, it resets to 5. \n",
    "\n",
    "So here is how you can use more categories for your choropleths: create a new column with the appropriate PySAL function and map that, as follows. By \"appropriate PySAL function\" we mean whichever classification algorithm you select, above we used Quantiles and below we are using the Fisher Jenks algorithm. The PySAL documentation is quite good so we encourage you check there (as well as the notebook from Julia Koschinsky's colleagues referenced above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the 'Fisher_Jenks' scheme:\n",
    "fj10 = ps.Fisher_Jenks(nhoods.tot_biz,k=10)\n",
    "\n",
    "# the ps.<scheme> function returns two things, the bins used for the cutoffs:\n",
    "print('bins:')\n",
    "print(fj10.bins)\n",
    "# and the assigned bin number to use:\n",
    "print('\\nbin number:')\n",
    "print(fj10.yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use the new categories to color the choropleth of land area into 10 buckets\n",
    "# notice the couple new keywords we include\n",
    "\n",
    "# again we'll create the matplotlib figure and axis objects first\n",
    "f, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# then create our choropleth, \n",
    "# the \"assign\" function adds our Fisher Jenks buckets as a new column to map\n",
    "## Note with this formulation 'cl' is a temporary column\n",
    "# the 'categorical=True' tells geopandas we want a different color for each category\n",
    "nhoods.assign(cl=fj10.yb).plot(column='cl', categorical=True, \\\n",
    "        cmap='viridis', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='grey', legend=True)\n",
    "\n",
    "# turn off the latitude/longitude axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above map suggests a few neighborhoods have more businesses than others, some of which appear to be in Chicago, however this brings us to an issue central to much of spatial analysis: do we actually care about the _total number of businesses_? Possibly. Often, however, we care more about **where there are _higher concentrations_ of our study variable as compared to some base value**, so next we will consider the rate of IDHS household cases by housing units for 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleth of rate\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**NOTE: BELOW THIS POINT IS YET TO BE UPDATED**\n",
    "\n",
    "The \"chi_nhood_hhcase2015\" table in the class2 schema was created with the following SQL:\n",
    "\n",
    "    SELECT n.neighborhood, sum(q.hh_counts) hhcase_total, n.geom\n",
    "    INTO class2.chi_nhood_hhcase2015\n",
    "    FROM chicago_nhoods AS n\n",
    "    JOIN (SELECT h.hh_counts, g.geom_2163 \n",
    "        FROM idhs.hh_indcase_spells h\n",
    "        JOIN idhs.case_geocode g\n",
    "        ON h.ch_dpa_caseid = g.ch_dpa_caseid\n",
    "        WHERE h.start_date >= '2015-01-01' AND h.start_date < '2016-01-01') q\n",
    "    ON ST_Within(q.geom_2163, n.geom_2163)\n",
    "    GROUP BY n.neighborhood, n.geom\n",
    "    ORDER BY n.neighborhood;\n",
    "    \n",
    "    \n",
    "    \n",
    "    ALTER TABLE class2.chi_nhood_estab_2014q3 ADD PRIMARY KEY (neighborhood);\n",
    "    CREATE INDEX chi_nhood_estab_2014q3_geom_gist ON class2.chi_nhood_estab_2014q3 USING gist (geom);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "qry =\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# get new data\n",
    "nhoods_hh = gpd.read_postgis(qry, engine, geom_col='geom', index_col='gid')\n",
    "\n",
    "# view info of returned GeoDataFrame\n",
    "nhoods_hh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the descriptive stats of the rate column?\n",
    "nhoods_hh.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nulls with 0 as we assume there are no ex-offenders who were released in 2015\n",
    "nhoods_hh[''].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again we'll use fisher-jenks with 10 classes\n",
    "fj10 = ps.Fisher_Jenks(nhoods_hh[''],k=10)\n",
    "\n",
    "# again we'll create the matplotlib figure and axis objects first\n",
    "f, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# then create our choropleth, the \"assign\" function adds our \n",
    "# Fisher Jenks buckets as a new column to map\n",
    "## Note as before with this formulation 'cl' is a temporary column\n",
    "# the 'categorical=True' tells geopandas we want a different color for each category\n",
    "nhoods_hh.assign(cl=fj10.yb).plot(column='cl', categorical=True, \\\n",
    "        cmap='viridis_r', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='grey', legend=True)\n",
    "\n",
    "# let's add a title to describe what we're plotting, the '\\n' adds a new line\n",
    "my_title = ''\n",
    "ax.set_title(my_title)\n",
    "\n",
    "# turn off the latitude/longitude axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleth multiple years\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "For something more interesting let's visualize..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for list of zipcodes:\n",
    "query = \"\"\"\"\"\"\n",
    "\n",
    "# get the zipcodes as a pandas dataframe\n",
    "zipcodes = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many zipcodes is this?\n",
    "len(zipcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert the zipcode dataframe into a string \n",
    "# with single quotes and commas separating each\n",
    "# to select out just those zipcodes in the following SQL query\n",
    "zipcodes = ','.join([\"'\"+z+\"'\" for z in zipcodes.zipcode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# almost our same query from above Choropleth of rate, with two differences\n",
    "# 1. get year from the exit_date and limit to our most recent 10 years\n",
    "# 2. we add year as a GROUP BY in the subquery\n",
    "qry = \"\"\"z.geoid10 IN ({});\"\"\".format(zipcodes)\n",
    "\n",
    "# get new data\n",
    "nhoods_hh_10yr = gpd.read_postgis(qry, engine, geom_col='geom', index_col='gid')\n",
    "\n",
    "# view info of returned GeoDataFrame\n",
    "nhoods_hh_10yr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view summary stats of full dataset\n",
    "nhoods_hh_10yr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a new column of quantiles to make comparing our years easier\n",
    "nhoods_hh_10yr['quant5'] = ps.Quantiles(ex_off_rate_10yr.exoffrate,k=5).yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each year, how many records are in each of our quantile buckets?\n",
    "grouped_counts = nhoods_hh_10yr.groupby(['year', 'quant5'])['zip5'].count().reset_index()\n",
    "\n",
    "# what does this dataframe look like?\n",
    "grouped_counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see it as a 10x5 table\n",
    "grouped_counts.pivot_table(index='year', columns='quant5', values='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2d list of years\n",
    "years = [range(2006, 2011, 1), range(2011, 2016, 1)]\n",
    "years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: following cell takes some time to run so it would be good if people attempt to coordinate in their groups to not all run at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll make a choropleth for each year all on the same figure\n",
    "# using mulitple subplots\n",
    "f, axes = plt.subplots(2, 5, sharex=True, sharey=True, figsize=(10,6))\n",
    "\n",
    "# do two for loops to plot each year in position\n",
    "for row in xrange(2):\n",
    "    for col in xrange(5):\n",
    "        temp_gdf = nhoods_hh_10yr[nhoods_hh_10yr.year==years[row][col]]\n",
    "#         print('temp gdf %s %s created' % (row, col))\n",
    "        temp_gdf.plot('quant5', categorical=True, \\\n",
    "            cmap='viridis_r', linewidth=0.1, ax=axes[row,col], \\\n",
    "            edgecolor='grey')\n",
    "        # also let's turn off the axis lables as they're not so useful here\n",
    "        axes[row,col].set_yticklabels([])\n",
    "        axes[row,col].set_xticklabels([])\n",
    "        # and set the title column as the year\n",
    "        axes[row,col].set_title(str(years[row][col]))\n",
    "# add a title for the whole figure\n",
    "f.suptitle('');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Addendum\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "### Addendum contents\n",
    "1. [Spatial data types](#Spatial-data-types)\n",
    "2. [Datasets](#Datasets) in this notebook\n",
    "2. Projections and [Coordinate Reference Systems (CRS)](#Coordinate-Reference-Systems)\n",
    "2. [Spatial info of tables in PostGIS](#Spatial-info-of-tables-in-PostGIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial data types\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "- Back to [Addendum contents](#Addendum-contents)\n",
    "\n",
    "There are two generic spatial data types:\n",
    "1. **Vector** - discrete data (usually), represented by points, lines, and polygons\n",
    "2. **Raster** - continuous data (usually), generally represented as square pixels where each pixel (or \"grid cell\") has some value. Examples of raster data - link to \"big data\"\n",
    "  * Imagery data (satellite, Google SteetView, traffic cameras, Placemeter)\n",
    "  * Surface data (collected at monitoring stations then interpolated to a 'surface' - eg Array of Things, weather data)\n",
    "  \n",
    "However, raster data is commonly used in few social science contexts, so the below image (courtesy of [Data Science for Social Good](https://github.com/geebioso/postgis-workshop/blob/master/tutorial.org)) is probably sufficiet discussion about rasters for now:\n",
    "![raster](images/raster_example.png)\n",
    "\n",
    "> Notice the pesky _\"usually\"_ next to both vector and raster datatypes? Technically any data **_could_** be represented as either vector or raster, but it would be computationally inefficient to create a raster layer of rivers or roads across Illinois because \n",
    "1. All the non-road and non-river locations would still have some value and \n",
    "2. You would have to pick a cell size which may not well represent the actual course of a given river (as opposed to a vector - line - that follows a path and could have some value for width of the river or road)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "- Back to [Addendum contents](#Addendum-contents)\n",
    "\n",
    "Datasets used in this Notebook\n",
    "1. US Counties - polygons of counties from US Census's TIGER\\Lines product (source-> https://www.census.gov/geo/maps-data/data/tiger-line.html)\n",
    "3. Chicago neighborhoods - polygons of Chicago's 88 neighborhoods created using the [Tract to Neighborhood](https://deepdish.adrf.info/detail/adrf-000073) lookup table to create polygons based on underlying Census Blocks. Code to create the neighborhood table is in [this notebook](https://jupyterhub.adrf.info:8000/user/crh278/notebooks/Projects/class2/user/crh278/class2_materials/data/Create-Chicago-neighborhood-spatial-table.ipynb)\n",
    "4. IDES addendum to employer locations, [link to ADRF Explorer page](https://deepdish.adrf.info/detail/adrf-000035)\n",
    "\n",
    "Considerations for spatial data\n",
    "+ Data collection - at what spatial scale were data collected?\n",
    "+ Data aggregation issue: aggregating to different spatial units could give different results due to something known as the \"modifiable areal unit problem\" (MAUP). MAUP is a statistical bias from summarizing point data to areas as the value of a given area depends on where the boundary is drawn (description paraphrased from Wikipedia, which has more info and links to related issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common type of vector data is called a \"shapefile\". Shapefiles are actually a collection of files and there are:\n",
    "1. Required files\n",
    "  + .dbf - contains attributes (ie variables that describe the object, like number of households in a zip code)\n",
    "  + .shp - the geographic information of the object\n",
    "  + .shx - an index to facilitate searching within data\n",
    "2. Additional / optional files\n",
    "  + .prj - the projetion or coordinate reference sysstem (CRS); technically optional but **highly** recommended as it tells you how the data relates to real-world locations\n",
    "  + .sbn / .sbx - other indexes used by specific software\n",
    "  + .shp.xml - metadata information\n",
    "  \n",
    "> Above description pulled mostly from the Wikipedia article on [shapefiles](https://en.wikipedia.org/wiki/Shapefile) which also has much more information.\n",
    "\n",
    "Other common vector formats:\n",
    "1. GeoJSON - Geospatial standards in the JavaScript Object Notation (JSON) text format\n",
    "2. KML / KMZ - Keyhole Markup Language popularized by Google\n",
    "3. GDB - Esri's Geodatabase, a proprietary data format used in ArcGIS software products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Reference Systems\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "- Back to [Addendum contents](#Addendum-contents)\n",
    "\n",
    "Coordinate Reference Systems (aka projections) are basically math that (1) describes how information in a given dataset relates to the rest of the world and (2) usually creates a 'flat' surface on which data can be analyzed using more common algorithms (eg Euclidean geometry). \n",
    "\n",
    ">Why do we care?\n",
    "1. Distance / area measurements\n",
    "2. Spatial join - won't work with different CRS\n",
    "\n",
    "\n",
    "As an example of point 2, consider the distance between two points: Euclidean distance (aka pythagorean theorem) provides an easy way to calculate distance so long as we know the difference in **_x_** (longitude) and **_y_** (latitude) between two points:\n",
    "$$Distance   = \\sqrt(({x}_1-{x}_2)^2 + ({y}_1-{y}_2)^2)$$\n",
    "\n",
    "This works fine on **_correctly projected_** data, but **_does not work_** on unprojected data. For one thing the result would be in degrees and degrees are a different distance apart (in terms of meters or miles) at different points on the Earth's surface.\n",
    "\n",
    "All this is to say: if you do a calculation with geographic data and the numbers don't make sense, check the projection. Let's do an example with the IL county areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DB connection (connection string is \"<database-type>://<username>@<host-url>:<port>/<database-name>\")\n",
    "engine = create_engine(\"postgresql://10.10.2.10:5432/appliedda\")\n",
    "\n",
    "# create SQL query - limit to just IL by using the state FIPS code of 17\n",
    "sql = \"SELECT * FROM tl_2016_us_county WHERE statefp = '17';\"\n",
    "\n",
    "# get data from the database\n",
    "il_counties = gpd.read_postgis(sql, engine, geom_col='geom', index_col='gid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the CRS of IL counties:\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so first, it needs to be set (I'd guess GeoPandas will appropriately set from a database in the future). If we look it up in the database we'll see that it's NAD83 (North American Datum 1983), which has the [EPSG](www.epsg.org) (European Petroleum Survey Group) code of 4269."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the counties crs to 4269\n",
    "il_counties.crs = {'init': u'epsg:4269'}\n",
    "\n",
    "# print it out\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out the area calculated using Pandas with NAD83\n",
    "il_counties['area_nad83'] = il_counties.geom.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the first 5 records' aland and calculated area with NAD83:\n",
    "il_counties.loc[:,('aland', 'area_nad83')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not the same. We can look for other projections at a number of websites, but a good one is [epsg.io](www.epsg.io). let's use the US National Atlas Equal Area projection (epsg=2163), which is a meters based equal area projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform aka re-project the data (use the \"inplace=True\" flag to perform the operation on this Geodataframe)\n",
    "il_counties.to_crs(epsg=2163, inplace=True)\n",
    "\n",
    "# print out the CRS to see it worked\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's calculate the area with the new CRS\n",
    "il_counties['area_2163'] = il_counties.geom.area\n",
    "\n",
    "# and again check the head() of the data, with all 3 area columns:\n",
    "il_counties.loc[:,('aland', 'area_nad83', 'area_2163')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if those small differences are just because we're only looking at land area, not full county area:\n",
    "il_counties['total_area'] = il_counties.aland + il_counties.awater\n",
    "\n",
    "# and recheck areas against total:\n",
    "il_counties.loc[:,('total_area', 'area_nad83', 'area_2163')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are still some differences between our newly calculated area ('area_2163') and the total area that came in the data ('aland' + 'awater'), however we can see it's much closer than the nad83 version. These small differences most likely mean that the area from Census was calculated using a different Coordinate Reference System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spatial info of tables in PostGIS\n",
    "\n",
    "- Back to the [Table of Contents](#Table-of-Contents)\n",
    "- Back to the [Addendum contents](#Addendum-contents)\n",
    "\n",
    "In PostGIS the `geometry_columns` view maintains information about what geometry data exists in the tables in the database, so there's a simple way to see _all_ the tables that have spatial data like so:\n",
    "\n",
    "    SELECT f_table_name\n",
    "    FROM geometry_columns\n",
    "    GROUP BY f_table_name\n",
    "    ORDER BY f_table_name;\n",
    "    \n",
    "And there is also a way to see what spatial columns exist in a given table, along with their SRID and datatype:\n",
    "\n",
    "    SELECT f_geometry_column, srid, type\n",
    "    FROM geometry_columns WHERE\n",
    "    f_table_name = 'tl_2016_us_zcta510';\n",
    "\n",
    "> Reminder: the SRID is the Spatial Reference Identifier, a unique code used in the `spatial_ref_sys` table as described in the Coordinate Reference System section above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
