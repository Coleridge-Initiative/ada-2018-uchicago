{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction to Spatial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Datasets](#Datasets) in this Notebook\n",
    "2. [Choropleth maps](#Choropleths)\n",
    "3. [Spatial weights](#Spatial-weights)\n",
    "2. [Exploratory Spatial Data Analysis](#Exploratory-spatial-data-analysis)\n",
    "    1. [Global spatial autocorrelation: Moran's I](#Global-spatial-autocorrelation:-Moran's-I)\n",
    "    2. [Local Measures of Spatial Association](#Local-Measures-of-Spatial-Association)\n",
    "\n",
    "[Addendum](#Addendum)\n",
    "1. Overview of [spatial data types](#Spatial-data-types)\n",
    "2. [Coordinate Reference Systems (aka projections)](#Coordinate-Reference-Systems)\n",
    "2. [Common vector data formats](#Common-vector-data-formats)\n",
    "\n",
    "\n",
    "### Reference material\n",
    "**NOTE: we recommend you refer to the [Spatial Queries in PostGIS](Spatial-Queries-in-PostGIS.ipynb) notebook for explanations of the underlying SQL used below. The Spatial Queries notebook also has the SQL used to create the \"il_des_subset_2014q3\" table.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Datasets used in this Notebook\n",
    "1. Chicago neighborhoods - polygons of Chicago's 88 neighborhoods created using the [Tract to Neighborhood](https://deepdish.adrf.info/detail/adrf-000073) lookup table to create polygons based on underlying Census Blocks. Code to create the neighborhood table is in [this notebook](../../data/Create-Chicago-neighborhood-spatial-table.ipynb)\n",
    "2. Business location and info for 2014Q3 derived from IDES [employer](https://deepdish.adrf.info/detail/adrf-000034) and [addendum to employer locations](https://deepdish.adrf.info/detail/adrf-000035)\n",
    "\n",
    "Considerations for spatial data\n",
    "+ Data collection - at what spatial scale were data collected?\n",
    "+ Data aggregation issue: aggregating to different spatial units could give different results due to something known as the \"modifiable areal unit problem\" (MAUP). MAUP is a statistical bias from summarizing point data to areas as the value of a given area depends on where the boundary is drawn (description paraphrased from Wikipedia, which has more info and links to related issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data manipulation libraries ##\n",
    "# Pandas for generic manipulation\n",
    "import pandas as pd\n",
    "# GeoPandas for spatial data manipulation\n",
    "import geopandas as gpd\n",
    "\n",
    "# PySAL for spatial statistics\n",
    "import pysal as ps\n",
    "\n",
    "# shapely for specific spatial data tasks (GeoPandas uses Shapely objects)\n",
    "# commented out as not used, but may be useful reference\n",
    "# from shapely.geometry import Point, LineString, Polygon\n",
    "\n",
    "# SQLAlchemy to get some data from the database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# improve control of visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up connection paramaters for the 'appliedda' database\n",
    "host = '10.10.2.10'\n",
    "db = 'appliedda'\n",
    "\n",
    "# create DB connection \n",
    "# (connection string is \"<database-type>://<username>@<host-url>:<port>/<database-name>\")\n",
    "engine = create_engine(\"postgresql://{host_str}/{db_str}\"\n",
    "                       .format(host_str = host, db_str=db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in point locations - the full file has ~455k locations (~208k of which are in Cook County)\n",
    "# so here we are going to grab a sample of 500 from within Cook County\n",
    "query = \"\"\"\n",
    "SELECT e.*\n",
    "\n",
    "FROM il_des_kcmo.il_des_establishment e\n",
    "JOIN tl_2016_us_county c\n",
    "ON ST_Within(e.geom, c.geom)\n",
    "\n",
    "WHERE c.geoid = '17031' -- only Cook County\n",
    "AND e.year = 2013 AND e.quarter = 3 -- only Q3 2014\n",
    "\n",
    "LIMIT 500;\n",
    "\"\"\"\n",
    "points = gpd.read_postgis(query, engine, geom_col='geom',\n",
    "                          crs={'init': u'epsg:4269'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what info is contained in this file?\n",
    "points.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the first row look like?\n",
    "# points.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point data alone is not that interesting, so we will also pull Chicago neighborhoods which were created using the below SQL - they already contain the summary of number of businesses (`tot_biz`), total wages paid (`total_wages`), and the total number of jobs (`jobs_tot`) for 2014Q3.\n",
    "\n",
    "    CREATE TEMP TABLE il_des_subset AS\n",
    "    SELECT a.ein || '-' || a.rptunitno || '-' || a.uiacctno AS uid,\n",
    "        a.geom, b.total_wages, \n",
    "        b.empl_month1::int + b.empl_month2::int + \n",
    "        b.empl_month3::int total_jobs\n",
    "    FROM il_des_kcmo.il_des_establishment a\n",
    "    JOIN il_des_kcmo.il_qcew_employers b\n",
    "    ON a.ein = b.ein AND a.rptunitno = b.seinunit \n",
    "        AND a.uiacctno = b.empr_no\n",
    "    WHERE b.multi_unit_code = '1'\n",
    "        AND a.year =2013 and a.quarter =3\n",
    "        AND b.year =2013 and b.quarter =3;\n",
    "\n",
    "\n",
    "    SELECT n.neighborhood, count(e.*) tot_biz, \n",
    "        sum(e.total_wages) wages_tot, sum(e.total_jobs) jobs_tot, n.geom \n",
    "    INTO ada_18_uchi.chi_nhood_estab_2014q3\n",
    "    \n",
    "    FROM chicago_nhoods AS n\n",
    "    LEFT JOIN il_des_subset e\n",
    "    ON ST_Within(e.geom, n.geom)\n",
    "    \n",
    "    GROUP BY n.neighborhood, n.geom\n",
    "    ORDER BY n.neighborhood;\n",
    "    \n",
    "    ALTER TABLE ada_18_uchi.chi_nhood_estab_2014q3 ADD PRIMARY KEY (neighborhood);\n",
    "    CREATE INDEX ada_18_uchi_chi_nhood_estab_2014q3_geom_gist ON ada_18_uchi.chi_nhood_estab_2014q3 USING gist (geom);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data \n",
    "qry = \"SELECT * FROM ada_18_uchi.chi_nhood_estab_2014q3\"\n",
    "\n",
    "nhoods = gpd.read_postgis(qry, engine, geom_col='geom', \n",
    "                          crs='+init=epsg:4269')\n",
    "nhoods.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try putting the sample of business locations on top of neighborhoods #\n",
    "\n",
    "# create a map of Chicago neighborhoods colored grey\n",
    "ax = nhoods.plot(color='grey', figsize=(6,8))\n",
    "# note we assign counties to the object \"ax\" so we can \n",
    "# overlay the points on the same \"matplotlib axis\"\n",
    "\n",
    "# use the same \"ax\" object to plot the prisons on top of the neighborhood map, \n",
    "# plus resize the markers and remove their outlines\n",
    "points.plot(ax=ax, markersize=5); \n",
    "plt.annotate('Source: IDES', xy=(0.8,-0.10), xycoords=\"axes fraction\")\n",
    "plt.title('Business locations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choropleths\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Choropleths are very useful because they can quickly convey how values compare across the study area. Let's start wtih a simple example of the number of businesses by neighborhood. (Note much of the code below comes from this Geovisualization (source -> http://darribas.org/gds_scipy16/ipynb_md/02_geovisualization.html) notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll create our matplotlib figure and axis first so we have more control over it later\n",
    "f, ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# we'll pass the geopandas plot() function the column, scheme (calculation method), \n",
    "# number of groups to calculate (k)\n",
    "# colormap to use, linewidge (to make the edges less noticeable),\n",
    "# the axis object created above, and include a legend\n",
    "nhoods.plot(column='tot_biz', scheme='QUANTILES', k=10, \n",
    "               cmap='plasma', linewidth=0.1, ax=ax, legend=True)\n",
    "\n",
    "# and this time we'll turn off the coordinates\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in older versions of Geopandas, only allow using the \"quantiles\" (or any other [scheme supported by PySAL -> http://pysal.readthedocs.io/en/latest/library/esda/mapclassify.html) with between 2 and 9 and if you try soemthing different, it resets to 5. \n",
    "\n",
    "So here is how you can use more categories for your choropleths: create a new column with the appropriate PySAL function and map that, as follows. By \"appropriate PySAL function\" we mean whichever classification algorithm you select, above we used Quantiles and below we are using the Fisher Jenks algorithm. In extreme brief, Fisher-Jenks is a classification method which maximizes the difference in means between groups while minimizing the variance within groups.\n",
    "\n",
    "The PySAL documentation is quite good so we encourage you check there (as well as the notebook referenced above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try the 'Fisher_Jenks' scheme:\n",
    "fj10 = ps.Fisher_Jenks(nhoods.tot_biz,k=10)\n",
    "\n",
    "# the ps.<scheme> function returns two things, the bins used for the cutoffs:\n",
    "print('bins:')\n",
    "print(fj10.bins)\n",
    "# and the assigned bin number to use:\n",
    "print('\\nbin number:')\n",
    "print(fj10.yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can use the new categories to color the choropleth of land area into 10 buckets\n",
    "# notice the couple new keywords we include\n",
    "\n",
    "# again we'll create the matplotlib figure and axis objects first\n",
    "f, ax = plt.subplots(figsize=(6, 8))\n",
    "\n",
    "# then create our choropleth, \n",
    "# the \"assign\" function adds our Fisher Jenks buckets as a new column to map\n",
    "## Note with this formulation 'cl' is a temporary column\n",
    "# the 'categorical=True' tells geopandas we want a different color for each category\n",
    "nhoods.assign(cl=fj10.yb).plot(column='cl', categorical=True, \\\n",
    "        cmap='viridis', linewidth=0.1, ax=ax, \\\n",
    "        edgecolor='grey', legend=True)\n",
    "\n",
    "# turn off the latitude/longitude axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1**: above is a choropleth of total businesses, which may or may not be a useful visualization. Try creating a choropleth of some other value that does not already exist in the `nhoods` GeoDataFrame (ie calculate a new value first and then visualize it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial weights\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In the next sections we will demonstrate Moran's I and Getis-Ord G - measures of spatial correlation and association, respectively. In order for PySAL to assess if neighboring values are correlated or associated we first need to tell PySAL what to consider a neighbor for any given polygon. Additionally we need to tell PySAL _how connected_ different neighbors are - this connectedness is generally termed \"weight\". These two components - neighbors and weights - are together known as \"spatial weights\".\n",
    "\n",
    "> Side note: this is a very similar concept to what we'll be talking about in the Network Analysis session.\n",
    "\n",
    "There are a number of different ways to construct weights and we encourage you to look at the PySAL documentation for a description of each. Here we will consider \"queen contiguity\" - so named because of how the chess pieces moves on a chess board (yes, there is also rook and bishop contiguity).\n",
    "\n",
    "PySAL gives us the ability to create weights directly from other objects, such as a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create weights, specificying 'geom' rather than default 'geometry' column\n",
    "w = ps.weights.Queen.from_dataframe(nhoods, geom_col='geom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the neighbor dictionary defines which areas are neighbors\n",
    "w.neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the entries in our dataframe associated with one set of neighbors:\n",
    "nhoods.loc[[77, 1, 36, 62, 46],'neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is also generally a good idea to standardize weights\n",
    "# which you can do by setting \"transform\"\n",
    "w.transform = 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory spatial data analysis\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The below code is sourced mostly from a notebook on Spatial Exploratory Data Analysis: http://darribas.org/gds_scipy16/ipynb_md/04_esda.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global spatial autocorrelation: Moran's I\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The great font of knowledge, Wikipedia, tells us \"Moran's I is a measure of spatial autocorrelation ... [which] is characterized by a correlation in a signal among nearby locations in space. Spatial autocorrelation is more complex than one dimensional autocorrelation because spatial correlation is multi-dimensional (ie 2 or 3 dimensions of space) and multi-directional.\"\n",
    "\n",
    "Using the spatial weights created above we can derive a \"spatial lag\" variable to better understand how Moran's I works. For these examples let's consider the average wages per job across neighborhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable\n",
    "nhoods['wages_per_job'] = nhoods['wages_tot'] / nhoods['jobs_tot']\n",
    "# descriptive stats of nhoods:\n",
    "nhoods.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the spatially lagged value:\n",
    "nhoods['AvgWagesLag'] = ps.lag_spatial(w, nhoods['wages_per_job'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's see a map of the quantiles for each side by side\n",
    "f, axes = plt.subplots(1, 2, sharey=True, figsize=(10,6))\n",
    "\n",
    "# map wages per job on first map\n",
    "nhoods.plot('wages_per_job', scheme='QUANTILES', k=7, \\\n",
    "    cmap='viridis_r', linewidth=0.1, ax=axes[0], \\\n",
    "    edgecolor='grey', legend=True)\n",
    "axes[0].set_title('Wages per job')\n",
    "# and spatially lagged value on second\n",
    "nhoods.plot('AvgWagesLag', scheme='QUANTILES', k=7,  \\\n",
    "    cmap='viridis_r', linewidth=0.1, ax=axes[1], \\\n",
    "    edgecolor='grey', legend=True)\n",
    "axes[1].set_title('Spatially lagged wages per job')\n",
    "\n",
    "f.suptitle('Study variable vs spatially lagged variable');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 2**: identify one reason the above visual is misleading - can you think of better visuals for the question we are trying to answer? (how the two variables compare)\n",
    "\n",
    "One way to compare the two variables is a more formal statistical measure, Moran's I, and one visualization of it is simply a scatterplot of the study variable vs the spatially lagged variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate slope and intercept with single polynomial least squares (ie OLS) \n",
    "b,a = pd.np.polyfit(nhoods['wages_per_job'], nhoods['AvgWagesLag'], 1)\n",
    "\n",
    "# create scatter plot\n",
    "plt.plot(nhoods['wages_per_job'], nhoods['AvgWagesLag'], '.b')\n",
    "\n",
    "# add plot labels\n",
    "plt.ylabel('Spatially lagged wages per job')\n",
    "plt.xlabel('Wages per job')\n",
    "plt.title('Moran scatterplot')\n",
    "\n",
    "# add mean lines\n",
    "plt.vlines(nhoods['wages_per_job'].mean(),\n",
    "           nhoods['AvgWagesLag'].min(), \n",
    "           nhoods['AvgWagesLag'].max(),'k', '--')\n",
    "plt.hlines(nhoods['AvgWagesLag'].mean(), \n",
    "           nhoods['wages_per_job'].min(), \n",
    "           nhoods['wages_per_job'].max(),'k', '--')\n",
    "\n",
    "# add red dashed line of best fit\n",
    "plt.plot(nhoods['wages_per_job'], a + b*nhoods['wages_per_job'], '--r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper right and lower left quadrants represent positive spatial correlation where the upper left and lower right are negative spatial correlation.\n",
    "\n",
    "Doing the same thing a different way, we can compute the global Moran's I in PySAL with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global Moran's I\n",
    "wagesPerJob_I = ps.Moran(nhoods['wages_per_job'], w)\n",
    "\n",
    "print(\"Global Moran's I is {:.4f} with a p-value of {:0.4f}\"\\\n",
    "      .format(wagesPerJob_I.I, wagesPerJob_I.p_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more you can explore here, but overall this tells us that yes our study variable exhibits spatial autocorrelation, so let's dig in to what that means at a local level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Measures of Spatial Association\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "**Local Moran's I** is, unsurprisingly, Moran's I adapted to compute a statistic for each polygon in our dataset rather than a single statistic for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate local statistic:\n",
    "WagesPerJob_LocI = ps.Moran_Local(nhoods['wages_per_job'], w, \n",
    "                                  permutations=9999)\n",
    "\n",
    "# how many of our computed values are statistically significant?\n",
    "(WagesPerJob_LocI.p_sim < 0.001).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very many of the local Moran's I statistics are significant even though our global statistic is - which simply means there are not many strong hotspots for our small dataset.\n",
    "\n",
    "> **Exercise 3**: compute local Moran's I for a different variable, for an extra challenge try using a different geographic unit than neighborhoods\n",
    "\n",
    "Let's make a map of our results, but instead of 99% confidence let's use 95%. First we will identify the \"hot\" and \"cold\" spots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify statistically significant results:\n",
    "sig_idx = WagesPerJob_LocI.p_sim < 0.05\n",
    "\n",
    "# identify hotspots using the quadrant attribute (see Moran scatterplot for brief explanation)\n",
    "# and only keep those that are statistically significant\n",
    "hotspots = WagesPerJob_LocI.q==1 * sig_idx\n",
    "\n",
    "# similarly identify coldspots\n",
    "coldspots = WagesPerJob_LocI.q==3 * sig_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot them #\n",
    "# first create a single variable of both hot and cold spots:\n",
    "hotcold = hotspots*1 + coldspots*2\n",
    "\n",
    "# we'll also make our own colormap with a new matplotlib library\n",
    "from matplotlib import colors\n",
    "hcmap = colors.ListedColormap(['grey', 'red', 'blue'])\n",
    "\n",
    "# initalize fig and ax objects\n",
    "f, ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# and map the hot and cold spots\n",
    "nhoods.assign(cl=hotcold).plot('cl', categorical=True, k=2, \n",
    "                               cmap=hcmap, linewidth=0.1, \n",
    "                               legend=True,\n",
    "                              ax=ax)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getis-ord G** is a second measure of local spatial association and is similarly easy to calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate local G\n",
    "WagesPerJob_LocG = ps.G_Local(nhoods['wages_per_job'], w, \n",
    "                              permutations=9999)\n",
    "\n",
    "# how many are significant?\n",
    "print('{} observations are statistically significant'.\\\n",
    "      format((WagesPerJob_LocG.p_sim < 0.001).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and similarly let's make a map highlighting significant values, \n",
    "# but we'll do it slightly differently\n",
    "f, ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# create index of significant and insignificant values at 95%\n",
    "idx_sig = WagesPerJob_LocG.p_sim < 0.05\n",
    "idx_ins = WagesPerJob_LocG.p_sim >= 0.05\n",
    "\n",
    "# first we'll add our G values - the '\\' just splits things to different lines\n",
    "nhoods.assign(G=WagesPerJob_LocG.Gs)\\\n",
    ".loc[idx_sig,:]\\\n",
    ".plot('G', cmap='viridis', scheme='QUANTILES', \n",
    "      k=3, linewidth=0.1, ax=ax, legend=True)\n",
    "\n",
    "# then add insignificant values in grey\n",
    "nhoods.loc[idx_ins,:].plot(color='grey', linewidth=0.1, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the local G\\* version looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate local G*\n",
    "WagesPerJob_LocGstar = ps.G_Local(nhoods['wages_per_job'], w, \n",
    "                                  permutations=9999, \n",
    "                                  star=True)\n",
    "\n",
    "# how many are significant?\n",
    "print('{} observations are statistically significant'.format((WagesPerJob_LocGstar.p_sim < 0.05).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Addendum\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "\n",
    "1. Overview of [spatial data types](#Spatial-data-types)\n",
    "2. Projections and [Coordinate Reference Systems (CRS)](#Coordinate-Reference-Systems)\n",
    "2. [Common vector data formats](#Common-vector-data-formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial data types\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "There are two generic spatial data types:\n",
    "1. **Vector** - discrete data (usually), represented by points, lines, and polygons\n",
    "2. **Raster** - continuous data (usually), generally represented as square pixels where each pixel (or \"grid cell\") has some value. Examples of raster data - link to \"big data\"\n",
    "  * Imagery data (satellite, Google SteetView, traffic cameras, Placemeter)\n",
    "  * Surface data (collected at monitoring stations then interpolated to a 'surface' - eg Array of Things, weather data)\n",
    "  \n",
    "However, raster data is commonly used in few social science contexts, so the below image (courtesy of [Data Science for Social Good](https://github.com/geebioso/postgis-workshop/blob/master/tutorial.org)) is probably sufficiet discussion about rasters for now:\n",
    "![raster](images/raster_example.png)\n",
    "\n",
    "> Notice the pesky _\"usually\"_ next to both vector and raster datatypes? Technically any data **_could_** be represented as either vector or raster, but it would be computationally inefficient to create a raster layer of rivers or roads across Illinois because \n",
    "1. All the non-road and non-river locations would still have some value and \n",
    "2. You would have to pick a cell size which may not well represent the actual course of a given river (as opposed to a vector - line - that follows a path and could have some value for width of the river or road)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Reference Systems\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Coordinate Reference Systems (aka projections) are basically math that (1) describes how information in a given dataset relates to the rest of the world and (2) usually creates a 'flat' surface on which data can be analyzed using more common algorithms (eg Euclidean geometry). \n",
    "\n",
    ">Why do we care?\n",
    "1. Distance / area measurements\n",
    "2. Spatial join - won't work with different CRS\n",
    "\n",
    "\n",
    "As an example of point 2, consider the distance between two points: Euclidean distance (aka pythagorean theorem) provides an easy way to calculate distance so long as we know the difference in **_x_** (longitude) and **_y_** (latitude) between two points:\n",
    "$$Distance   = \\sqrt(({x}_1-{x}_2)^2 + ({y}_1-{y}_2)^2)$$\n",
    "\n",
    "This works fine on **_correctly projected_** data, but **_does not work_** on unprojected data. For one thing the result would be in degrees and degrees are a different distance apart (in terms of meters or miles) at different points on the Earth's surface.\n",
    "\n",
    "All this is to say: if you do a calculation with geographic data and the numbers don't make sense, check the projection. Let's do an example with the IL county areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DB connection (connection string is \"<database-type>://<username>@<host-url>:<port>/<database-name>\")\n",
    "engine = create_engine(\"postgresql://10.10.2.10:5432/appliedda\")\n",
    "\n",
    "# create SQL query - limit to just IL by using the state FIPS code of 17\n",
    "sql = \"SELECT * FROM tl_2016_us_county WHERE statefp = '17';\"\n",
    "\n",
    "# get data from the database\n",
    "il_counties = gpd.read_postgis(sql, engine, geom_col='geom', index_col='gid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the CRS of IL counties:\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so first, it needs to be set (I'd guess GeoPandas will appropriately set from a database in the future). If we look it up in the database we'll see that it's NAD83 (North American Datum 1983), which has the [EPSG](www.epsg.org) (European Petroleum Survey Group) code of 4269."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the counties crs to 4269\n",
    "il_counties.crs = {'init': u'epsg:4269'}\n",
    "\n",
    "# print it out\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out the area calculated using Pandas with NAD83\n",
    "il_counties['area_nad83'] = il_counties.geom.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the first 5 records' aland and calculated area with NAD83:\n",
    "il_counties.loc[:,('aland', 'area_nad83')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly not the same. We can look for other projections at a number of websites, but a good one is [epsg.io](www.epsg.io). let's use the US National Atlas Equal Area projection (epsg=2163), which is a meters based equal area projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform aka re-project the data (use the \"inplace=True\" flag to perform the operation on this Geodataframe)\n",
    "il_counties.to_crs(epsg=2163, inplace=True)\n",
    "\n",
    "# print out the CRS to see it worked\n",
    "print(il_counties.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's calculate the area with the new CRS\n",
    "il_counties['area_2163'] = il_counties.geom.area\n",
    "\n",
    "# and again check the head() of the data, with all 3 area columns:\n",
    "il_counties.loc[:,('aland', 'area_nad83', 'area_2163')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if those small differences are just because we're only looking at land area, not full county area:\n",
    "il_counties['total_area'] = il_counties.aland + il_counties.awater\n",
    "\n",
    "# and recheck areas against total:\n",
    "il_counties.loc[:,('total_area', 'area_nad83', 'area_2163')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are still some differences between our newly calculated area ('area_2163') and the total area that came in the data ('aland' + 'awater'), however we can see it's much closer than the nad83 version. These small differences most likely mean that the area from Census was calculated using a different Coordinate Reference System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Common vector data formats\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common type of vector data is called a \"shapefile\". Shapefiles are actually a collection of files (eg the `IL_prisons.*` files above) and there are:\n",
    "1. Required files\n",
    "  + .dbf - contains attributes (ie variables that describe the object, like number of households in a zip code)\n",
    "  + .shp - the geographic information of the object\n",
    "  + .shx - an index to facilitate searching within data\n",
    "2. Additional / optional files\n",
    "  + .prj - the projetion or coordinate reference sysstem (CRS); technically optional but **highly** recommended as it tells you how the data relates to real-world locations\n",
    "  + .sbn / .sbx - other indexes used by specific software\n",
    "  + .shp.xml - metadata information\n",
    "  \n",
    "> Above description pulled mostly from the Wikipedia article on [shapefiles](https://en.wikipedia.org/wiki/Shapefile) which also has much more information.\n",
    "\n",
    "Other common vector formats:\n",
    "1. GeoJSON - Geospatial standards in the JavaScript Object Notation (JSON) text format\n",
    "2. KML / KMZ - Keyhole Markup Language popularized by Google\n",
    "3. GDB - Esri's Geodatabase, a proprietary data format used in ArcGIS software products"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
