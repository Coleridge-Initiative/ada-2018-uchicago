{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECORD LINKAGE: EXCERCISE AND ASSIGNMENT\n",
    "\n",
    "----\n",
    "\n",
    "This notebook will provide you with an instruction into Record Linkage using Python. Upon completion of this notebook you will be able to apply record linkage techniques using the *recordlinkage* package to combine data from different sources in Python. \n",
    "It will lead you through all the steps necessary for a sucessful record linkage starting with data preparation  including pre-processing, cleaning and standardization of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [The Principles of Record Linkage](#The-Principles-of-Record-Linkage)\n",
    "- [Linking Patents to Grants](#Linking-Patents-to-Grants)\n",
    "- [Import of Packages](#Import-of-Packages)\n",
    "- [Getting Grant and Patent Info](#Getting-Grant-and-Patent-Info)\n",
    "- [The Importance of Pre-Processing](#The-Importance-of-Pre-Processing)\n",
    "- [Record Linkage](#Record-Linkage)\n",
    "- [References and Further Readings](#References-and-Further-Readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principles of Record Linkage\n",
    "The goal of record linkage is to determine if pairs of records describe the same identity. For instance, this is important for removing duplicates from a data source or joining two separate data sources together. Record linkage also goes by the terms data matching, merge/purge, duplication detection, de-duping, reference matching, entity resolution, disambiguation, co-reference/anaphora in various fields.\n",
    "\n",
    "There are several approaches to record linkage that include \n",
    "    - exact matching, \n",
    "    - rule-based linking and \n",
    "    - probabilistic linking. \n",
    "- An example of **exact matching** is joining records based on a direct identifier. This is we have already have done in SQL by joining tables in the last lecture and lab. \n",
    "- **Rule-based matching** involves applying a cascading set of rules that reflect the domain knowledge of the - records being linked. \n",
    "- In **probabilistic record linkages**, linkage weights are estimated to calculate the probability of a certain match.\n",
    "\n",
    "In practical applications you will need record linkage techiques to combine information addressing the same entity that is stored in different data sources. Record linkage will also help you to address the quality of different data sources. For example, if one of your databases has missing values you might be able to fill those by finding an identical pair in a different data source. Overall, the main applications of record linkage are\n",
    "    1. Merging two or more data files \n",
    "    2. Identifying the intersection of the two data sets \n",
    "    3. Updating data files (with the data row of the other data files) and imputing missing data\n",
    "    4. Entity disambiguation and de-duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Patents to Grants\n",
    "\n",
    "In this lab we will link records obtained from the patent data base (http://www.patentsview.org/web/#viz/relationships) to the data on federal grants we have in our database. In both datasets we have names that we can use to link. We have the PI on the grants and inventor names on patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of Packages\n",
    "Python provides us with some tools we can use for record linkages so we don't have to start from scratch and code our own linkage algorithms. So before we start we need to load the package recordlinkage. To fully function this packages uses other packages which also need to be imported. Thus we are adding more packages to the ones you are already familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general use imports\n",
    "%pylab inline\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import six\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# pandas-related imports\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "# record linkage package\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import clean, phonenumbers, phonetic\n",
    "\n",
    "# CSV file reading-related imports\n",
    "import csv\n",
    "\n",
    "# sqlalchemy an psycopg2 are sql connection packages\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "print( \"Imports loaded at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Grant and Patent Info\n",
    "We can get the data we need from our database and save the the required information in a dataframe. We therefore have to connect to the database and run a SQL query, as we did in the last session. This is what you will do in your assignment. In the lab we will prepare the Patent info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Inventor Data\n",
    "inventor=pd.read_csv('~/FederalReporter/inventor.csv',error_bad_lines=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the attributes of the variables in the loaded dataframe\n",
    "inventor.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the data frame\n",
    "inventor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying missing values by using the describe function\n",
    "inventor.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at the names which have the maximum frequencies associated with them\n",
    "inventor['name_last'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Pre-Processing\n",
    "Data pre-processing is an important step in a data anlysis project in general, in record linkage applications in particular. The goal of pre-processing is to transform messy data into a dataset that can be used in a project workflow.\n",
    "\n",
    "Linking records from different data sources comes with different challenges that need to be addressed by the analyst. The analyst must determine whether or not two entities (individuals, businesses, geographical units) on two different files are the same. This determination is not always easy. In most of the cases there is no common uniquely identifing characteristic for a entity. For example, is Bob Miller from New Yor the same person as Bob Miller from Chicago in a given dataset? This detemination has to be executed carefully because consequences of wrong linkages may be substantial (is person X the same person as the person X on the list of identified terrorists). Pre-processing can help to make better informed decisions.\n",
    "\n",
    "Pre-processing can be difficult because there are a lot of things to keep in mind. For example, data input errors, such as typos, misspellings, truncation, abbreviations, and missing values need to be corrected. Literature shows that preprocessing can improve matches. In some situations, 90% of the improvement in matching efficiency may be due to preprocessing. The most common reason why matching projects fail is lack of time and resources for data cleaning. \n",
    "\n",
    "In the following we will walk you through some pre-processing steps, these include but are not limited to removing spaces, parsing fields, and standardizing strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parsing String Variables\n",
    "\n",
    "By default, the split method returns a list of strings obtained by splitting the original string on spaces or commas, etc. The record linkage package comes with a build in cleaning function we can also use. In addition, we can extract information from strings for example by using regex search commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Regular Expressions - regex\n",
    "When defining a regular expression search pattern, it is a good idea to start out by writing down, explicitly, in plain English, what you are trying to search for and exactly how you identify when you've found a match.\n",
    "For example, if we look at an author field formatted as \"<last_name> , <first_name> <middle_name>\", in plain English, this is how I would explain where to find the last name: \"starting from the beginning of the line, take all the characters until you see a comma.\"\n",
    "\n",
    "We can build a regular expression that captures this idea from the following components:\n",
    "- ^ Matches beginning of the line\n",
    "- . Matches any character\n",
    "- .+ A modifier that means \"match one or more of the preceding expression\"\n",
    "\n",
    "In a regular expression, there are special reserved characters and character classes like those in the list above. Anything that is not a special character or class is just looked for explicitly (for example, a comma is not a special character in regular expressions, so if it is in a regular expression pattern, the regular expression processor will just be looking for a comma in the string, at that point in the pattern).\n",
    "\n",
    "Note: if you want to actually look for one of these reserved characters, it must be escaped, so that, for example, the expression looks for a literal period, rather than the special regular expression meaning of a period. To escape a reserved character in a regular expression, precede it with a back slash ( \".\" ).\n",
    "This results in the regular expression: ^.+,\n",
    "\n",
    "We start at the beginning of the line ( \"^\" ), matching any characters ( \".+\" ) until we come to the literal character of a comma ( \",\" ).\n",
    "\n",
    "In python, to use a regular expression like this to search for matches in a given string, we use the built-in \"re\" package ( https://docs.python.org/2/library/re.html ), specifically the \"re.search()\" method. To use \"re.search()\", pass it first the regular expression you want to use to search, enclosed in quotation marks, and then the string you want to search within. \n",
    "\n",
    "#### REGEX CHEATSHEET\n",
    "\n",
    "\n",
    "    - abc...     Letters\n",
    "    - 123...     Digits\n",
    "    - \\d         Any Digit\n",
    "    - \\D         Any non-Digit Character\n",
    "    - .          Any Character\n",
    "    - \\.         Period\n",
    "    - [a,b,c]    Only a, b or c\n",
    "    - [^a,b,c]   Not a,b, or c\n",
    "    - [a-z]      Characters a to z\n",
    "    - [0-9]      Numbers 0 to 9\n",
    "    - \\w any     Alphanumeric chracter\n",
    "    - \\W         any non-Alphanumeric character\n",
    "    - {m}        m Repetitions\n",
    "    - {m,n}      m to n repetitions\n",
    "    - *          Zero or more repetitions\n",
    "    - +          One or more repetitions\n",
    "    - ?          Optional Character\n",
    "    - \\s         any Whitespace\n",
    "    - \\S         any non-Whitespace character\n",
    "    - ^...$      Starts & Ends\n",
    "    - (...)      Capture Group\n",
    "    - (a(bc))    Capture sub-Group\n",
    "    - (.*)       Capture All\n",
    "    - (abc|def)  Capture abc or def\n",
    "     \n",
    "#### EXAMPLES\n",
    "    - (\\d\\d|\\D) will match 22X, 23G, 56H, etc...\n",
    "    - \\w will match any characters between 0-9 or a-z\n",
    "    - \\w{1-3} will match any alphanumeric character of a length of 1 to 3. \n",
    "    - (spell|spells) will match spell or spells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Inventor Data\n",
    "Now we will clean and preprocess the inventor Data. We need to remove whitespaces, make sure that everything is in upper case, we need to parse the first name and the last name, and harmonize all the other information we need for the linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Whitespaces\n",
    "inventor.rename(columns=lambda x: x.strip(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning names (using the record linkage package tool, see imports)\n",
    "# Clean removes any characters such as '-', '.', '/', '\\', ':', brackets of all types. \n",
    "inventor['name_last']=clean(inventor['name_last'], lowercase=False, remove_brackets=False)\n",
    "inventor['name_first']=clean(inventor['name_first'], lowercase=False, remove_brackets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the first character in the cleaned name variable\n",
    "inventor['name_middle'] = inventor.name_first.str.split(' ').str.get(1)\n",
    "inventor['name_first'] = inventor.name_first.str.split(' ').str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upcasing names\n",
    "inventor['name_last']=inventor.name_last.str.upper()\n",
    "inventor['name_first']=inventor.name_first.str.upper()\n",
    "inventor['name_middle']=inventor.name_middle.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventor.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with the inital data prep work for the inventor file. Please keep in mind that we just provided some examples for you to demonstrate the process. You can add as many further steps to it as necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Grant Data\n",
    "Now we will clean and preprocess the grants Data. This will be your assignment. We need clean names, zipcode, and a year variable. You can start working on this when we are done with the linkage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOU CAN INSERT THE CODE HERE OR GENERATE A NEW NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Prepared Grant Data\n",
    "grants=pd.read_csv('~/FederalReporter/grants.csv',error_bad_lines=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Linkage\n",
    "The record linkage package is a quite powerful tool for you to use when you want to link records within a dataset or across multiple datasets. It comes with different bulid in distances metrics and comparison functions, however, it also allows you to create your own. In general record linkage is divided in several steps. \n",
    "We've already done the pre-processing. We will add one more thing: a soundex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The phonetic() function is used to convert strings into their corresponding phonetic codes. \n",
    "## This is particularly useful when comparing names where different possible spellings make it difficult to find \n",
    "## exact matches (Ex. Jillian and Gillian).\n",
    "\n",
    "grants[\"phonetic_first\"] = phonetic(grants[\"name_first\"], method=\"nysiis\")\n",
    "grants[\"phonetic_last\"] = phonetic(grants[\"name_last\"], method=\"nysiis\")\n",
    "\n",
    "inventor[\"phonetic_first\"] = phonetic(inventor[\"name_first\"], method=\"nysiis\")\n",
    "inventor[\"phonetic_last\"] = phonetic(inventor[\"name_last\"], method=\"nysiis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Indexing allows you to create candidate links, which basically means identifying pairs of data rows which might refer to the same real world entity. This is also called the comparison space (matrix). There are different ways to index data. The easiest is to create a full index and consider every pair a match. This is also the least efficient method, because we will be comparing every row of one dataset with every row of the other dataset.\n",
    "\n",
    "If we had 10,000 records in data frame A and 100,000 records in data frame B, we would have 1,000,000,000 candidate links. You can see that comparing over a full index is getting inefficient when working with big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate a full index first (comparison table of all possible linkage combinations)\n",
    "#indexer = rl.FullIndex()\n",
    "#pairs = indexer.index(grants, inventor)\n",
    "# Returns a pandas MultiIndex object\n",
    "## How many records do we have?\n",
    "print (len(grants), len(inventor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better if we actually include our knowledge about the data to eliminate bad link from the start. This can be done through blocking. The recordlinkage packages gives you multiple options for this. For example, you can block by using variables, which menas only links exactly equal on specified values will be kept. You can also use a neighbourhood index in which the rows in your dataframe are ranked by some value and python will only link between the rows that are closeby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexerBL = rl.BlockIndex(on=['name_first', 'name_last'])\n",
    "pairs2 = indexerBL.index(grants, inventor)\n",
    "# Returns a pandas MultiIndex object\n",
    "print(len(pairs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Comparison\n",
    "\n",
    "After you have created a set of candidate links, you’re ready to begin comparing the records associated with each candidate link. In recordlinkage you must initiate a Compare object prior to performing any comparison functionality between records. This object stores both dataframes, the candidate links, and a vector containing comparison results. Further, the Compare object contains the methods for performing comparisons. The code block below initializes the comparison object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate compare object (we are using the blocked ones here)\n",
    "# You want to give python the name of the MultiIndex and the names of the datasets\n",
    "compare_cl = rl.Compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are five specific comparison methods within recordlinkage: Compare.exact(), Compare.string(), Compare.numeric(), Compare.geo(), and Compare.date(). The Compare.exact() method is simple: if two values are an exact match a comparison score of 1 is returned, otherwise 0 is retured. The Compare.string() method is a bit more complicated and generates a score based on well known string-comparison algorithms (for this example Levenshtein or Jaro Winkler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_cl.string('name_first', 'name_first', method='jarowinkler', threshold=0.85, label='name_first')\n",
    "compare_cl.string('name_last', 'name_last', method='jarowinkler', threshold=0.85, label='name_last')\n",
    "\n",
    "compare_cl.string('phonetic_first', 'phonetic_first', method='jarowinkler', threshold=0.85, label='phonetic_first')\n",
    "compare_cl.string('phonetic_last', 'phonetic_last', method='jarowinkler', threshold=0.85, label='phonetic_last')\n",
    "\n",
    "compare_cl.exact('organization_country', 'country', label='country')\n",
    "compare_cl.exact('fy', 'fy', label='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The comparing of record pairs starts when the compute method is called. \n",
    "## All attribute comparisons are stored in a DataFrame with horizontally the features and vertically the record pairs.\n",
    "\n",
    "features = compare_cl.compute(pairs2, grants, inventor)\n",
    "features.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Now we have to decide which records belong to one person. We can do this pretty simple, but we can also use machine learning methods to classify records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Classification: Check for how many attributes records are identical by summing the comparison results.\n",
    "features.sum(axis=1).value_counts().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = features[features.sum(axis=1) > 4]\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the list of matches we can fuse our dataset, becasue at the end we want to have a combined dataset. We are using a function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse(dfA, dfB, dfmatches):\n",
    "    newDF = dfA.copy()\n",
    "    columns = dfB.columns.values\n",
    "    \n",
    "    for col in columns:\n",
    "        newDF[col] = newDF.apply(lambda _: '', axis=1)\n",
    "        \n",
    "    for row in dfmatches.iterrows():\n",
    "        indexA = row[0][0]\n",
    "        indexB = row[0][1]\n",
    "        \n",
    "        for col in columns:\n",
    "            newDF.loc[indexA][col] = dfB.loc[indexB][col]\n",
    "    return newDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = fuse(grants, inventor, matches)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fellegi Sunter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this with a machine learning classifier. Keep in mind that most classifiers can not handle comparison vectors with missing values. To prevent issues with the classification algorithms. Supervised learning algorithms do need training data. Training data is data for which the true match status is known for each comparison vector. In the example in this section, we consider that the true match status of the first 5000 record pairs of our data is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Training Data and index\n",
    "ml_pairs = matches[0:40000]\n",
    "ml_matches_index = ml_pairs.index & pairs2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Naive Bayes classifier is a probabilistic classifier. The probabilistic record linkage framework by Fellegi and Sunter (1969) is the most well-known probabilistic classification method for record linkage. Later, it was proved that the Fellegi and Sunter method is mathematically equivalent to the Naive Bayes method in case of assuming independence between comparison variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the classifier\n",
    "nb = rl.NaiveBayesClassifier()\n",
    "nb.learn(ml_pairs, ml_matches_index)\n",
    "\n",
    "## Predict the match status for all record pairs\n",
    "result_nb = nb.predict(matches)\n",
    "\n",
    "## Predict probability for record to be a match\n",
    "prob_nb = nb.prob(matches)## Check header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check header\n",
    "prob_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The last step is to evaluate the results of the record linkage. We will cover this in more detail in the machine learning session. This is just for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix\n",
    "conf_nb = rl.confusion_matrix(pairs2, result_nb, len(matches))\n",
    "conf_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision and Accuracy\n",
    "precision = rl.precision(conf_nb)\n",
    "accuracy = rl.accuracy(conf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Precision and Accuracy\n",
    "print(precision)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The F-score for this classification is\n",
    "rl.fscore(conf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References and Further Readings\n",
    "\n",
    "\n",
    "### Parsing\n",
    "\n",
    "* Python online documentation: https://docs.python.org/2/library/string.html#deprecated-string-functions\n",
    "* Python 2.7 Tutorial(Splitting and Joining Strings): http://www.pitt.edu/~naraehan/python2/split_join.html\n",
    "\n",
    "### Regular Expression\n",
    "\n",
    "* Python documentation: https://docs.python.org/2/library/re.html#regular-expression-syntax\n",
    "* Online regular expression tester (good for learning): http://regex101.com/\n",
    "\n",
    "### String Comparators\n",
    "\n",
    "* GitHub page of jellyfish: https://github.com/jamesturk/jellyfish\n",
    "* Different distances that measure the differences between strings:\n",
    "    - Levenshtein distance: https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "    - Damerau–Levenshtein distance: https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance\n",
    "    - Jaro–Winkler distance: https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n",
    "    - Hamming distance: https://en.wikipedia.org/wiki/Hamming_distance\n",
    "    - Match rating approach: https://en.wikipedia.org/wiki/Match_rating_approach\n",
    "\n",
    "### Fellegi-Sunter Record Linkage \n",
    "\n",
    "* Introduction to Probabilistic Record Linkage: http://www.bristol.ac.uk/media-library/sites/cmm/migrated/documents/problinkage.pdf\n",
    "* Paper Review: https://www.cs.umd.edu/class/spring2012/cmsc828L/Papers/HerzogEtWires10.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "568px",
    "left": "0px",
    "right": "1125px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
