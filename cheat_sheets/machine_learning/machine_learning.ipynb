{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Quick Start\n",
    "\n",
    "# Table of contents\n",
    "\n",
    "- [Setup](#Setup)\n",
    "\n",
    "    - [Setup - Connect to the database](#Setup---Connect-to-the-database)\n",
    "    \n",
    "        - [Setup - Database - SQLAlchemy](#Setup---Database---SQLAlchemy)\n",
    "        - [Setup - Database - psycopg2](#Setup---Database---psycopg2)\n",
    "        - [Setup - Database - rollback if needed](#Setup---Database---rollback-if-needed)\n",
    "        \n",
    "- [Load data table](#Load-data-table)\n",
    "\n",
    "    - [Database access examples](#Database-access-examples)\n",
    "    - [Load your features and labels table](#Load-your-features-and-labels-table)\n",
    "\n",
    "- [Data Check](#Data-Check)\n",
    "\n",
    "    - [Look for null values](#Look-for-null-values)\n",
    "    - [Examine values within columns](#Examine-values-within-columns)\n",
    "    - [Examine distribution of key variables](#Examine-distribution-of-key-variables)\n",
    "\n",
    "- [Model Fitting](#Model-Fitting)\n",
    "\n",
    "    - [Make training and testing data](#Make-training-and-testing-data)\n",
    "    \n",
    "        - [Specify features and labels](#Specify-features-and-labels)\n",
    "        - [Split into training and testing sets using scikit-learn](#Split-into-training-and-testing-sets-using-scikit-learn)\n",
    "    \n",
    "            - [OPTIONAL - free up memory](#OPTIONAL---free-up-memory)\n",
    "\n",
    "        - [Create training and testing sets manually](#Create-training-and-testing-sets-manually)\n",
    "    \n",
    "    - [Model Selection](#Model-Selection)\n",
    "    - [Model Understanding](#Model-Understanding)\n",
    "\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "\n",
    "    - [Predicted vs. Expected](#Predicted-vs.-Expected)\n",
    "    - [Confusion Matrix](#Confusion-Matrix)\n",
    "    - [Accuracy](#Accuracy)\n",
    "    - [Precision and Recall](#Precision-and-Recall)\n",
    "    - [Precision and Recall at k percent](#Precision-and-Recall-at-k-percent)\n",
    "    - [Baseline](#Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import gc\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Connect to the database\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# schema name\n",
    "schema_name = \"\"\n",
    "\n",
    "# ==> database table names - just like file names above, store reused database information in variables here.\n",
    "\n",
    "# work table name\n",
    "work_db_table = \"\"\n",
    "\n",
    "print( \"Database variables initialized at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Database connection properties\n",
    "db_host = \"10.10.2.10\"\n",
    "db_port = -1\n",
    "db_username = None\n",
    "db_password = None\n",
    "db_name = \"appliedda\"\n",
    "\n",
    "print( \"Database connection properties initialized at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - Database - `SQLAlchemy`\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Initialize database connections.  First, SQLAlchemy engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize database connections\n",
    "# Create connection to database using SQLAlchemy\n",
    "#     (3 '/' indicates use enviroment settings for username, host, and port)\n",
    "sqlalchemy_connection_string = \"postgresql://\"\n",
    "\n",
    "if ( ( db_host is not None ) and ( db_host != \"\" ) ):\n",
    "    sqlalchemy_connection_string += str( db_host )\n",
    "#-- END check to see if host --#\n",
    "\n",
    "sqlalchemy_connection_string += \"/\"\n",
    "\n",
    "if ( ( db_name is not None ) and ( db_name != \"\" ) ):\n",
    "    sqlalchemy_connection_string += str( db_name )\n",
    "#-- END check to see if host --#\n",
    "\n",
    "# create engine.\n",
    "pgsql_engine = sqlalchemy.create_engine( sqlalchemy_connection_string )\n",
    "\n",
    "print( \"SQLAlchemy engine created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - Database - `psycopg2`\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "And then a direct psycopg2 connection and cursor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create psycopg2 connection to Postgresql\n",
    "\n",
    "# example connect() call that uses all the possible parameters\n",
    "#pgsql_connection = psycopg2.connect( host = db_host, port = db_port, database = db_name, user = db_username, password = db_password )\n",
    "\n",
    "# for SQLAlchemy, just needed database name. Same for DBAPI?\n",
    "pgsql_connection = psycopg2.connect( host = db_host, database = db_name )\n",
    "\n",
    "print( \"Postgresql connection to database \\\"\" + db_name + \"\\\" created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a cursor that maps column names to values\n",
    "pgsql_cursor = pgsql_connection.cursor( cursor_factory = psycopg2.extras.DictCursor )\n",
    "\n",
    "print( \"Postgresql cursor for database \\\"\" + db_name + \"\\\" created at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - Database - rollback if needed\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# rollback, in case you need it.\n",
    "pgsql_connection.rollback()\n",
    "\n",
    "print( \"Postgresql connection for database \\\"\" + db_name + \"\\\" rolled back at \" + str( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data table\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "For this quick start, we assume that there is a table that contains the items you want to analyze with a row per column and a column per feature or label.  This table can contain multiple columns you'd like to use for labels.  You can either filter to a given set of features and labels in the SQL you use to load the data, or you can filter later, when you break out the data in X and Y training and testing data frames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database access examples\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database connection allows us to use queries of a database to populate pandas DataFrames in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create SQL query\n",
    "sql_select = \"\"\"SELECT table_schema, table_name\n",
    "FROM information_schema.tables\n",
    "order by table_schema, table_name;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the data into a DataFrame (df)\n",
    "df_tables = pd.read_sql( sql_select, pgsql_engine )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at a few sample rows.\n",
    "df_tables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your features and labels table\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Now, we'll load our table that contains features and labels into a pandas DataFrame.\n",
    "\n",
    "First we create a SELECT statement.  This can be very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build SELECT to pull in features and labels table.\n",
    "sql_select = \"SELECT *\"\n",
    "sql_select += \" FROM \" + schema_name + \".\" + work_db_table\n",
    "sql_select += \";\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, it can be more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build SELECT to pull in features and labels table.\n",
    "sql_select = \"SELECT feature1, feature2, feature3, feature4, label1, label2\"\n",
    "sql_select += \" FROM \" + schema_name + \".\" + work_db_table\n",
    "sql_select += \" WHERE important_variable IS NOT NULL\"\n",
    "sql_select += \" AND age > 18\"\n",
    "sql_select += \";\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the data into a DataFrame (df)\n",
    "data_table_df = pd.read_sql( sql_select, pgsql_engine )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Check\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Now, we look at the columns in our data set to see if they are appropriate for machine learning models, and if not, we fix them.  Examples of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# first, look at small sample of rows.\n",
    "data_table_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for null values\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Machine learning models might or might not be able to accommodate null values in features or labels.  It is good to be aware of where nulls are in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get a list of the rows in the data that have empty values (known as NaN or null).\n",
    "isnan_rows_list = data_table_df.isnull().any( axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the contents of these rows.\n",
    "data_table_df[ isnan_rows_list ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at percent of rows that contain nulls\n",
    "nrows_data_table = data_table_df.shape[ 0 ]\n",
    "nrows_data_table_isnan = data_table_df[ isnan_rows_list ].shape[ 0 ]\n",
    "percent_isnan = float( nrows_data_table_isnan) / nrows_data_table\n",
    "print( '% of frows with NaNs {} '.format( str( percent_isnan ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, machine leraning doesn't like nulls.  So, we remove the rows with nulls/`NaN`s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_table_df = data_table_df[ ~isnan_rows_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine values within columns\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the values of a column to see if they are reasonable.  Our example: \"age\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# column name\n",
    "column_name = \"age\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we grab the unique values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use numpy to get unique values in a given column.\n",
    "np.unique( data_table_df[ column_name ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say, in our table, there are ages of 0 or less.  This is unlikely if these are people, for example.  So, let's drop any rows that have age less than 0 or greater than 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a filter criteria, then apply it to our data.\n",
    "filter_criteria = ~( ( data_table_df[ column_name ] < 1) | ( data_table_df[ column_name ] > 150 ) )\n",
    "\n",
    "# only keep rows from our DataFrame that fit our criteria.\n",
    "data_table_df = data_table_df[ filter_criteria ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine distribution of key variables\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "As we clean up, we should intermittently check how much data we still have and how key variables of interest are distributed. We don't necessarily need to have a perfect balance in any given feature or label, but it's good to know what the \"baseline\" is in our dataset, to be able to intelligently evaluate our performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# number of rows:\n",
    "print('Number of rows: {}'.format( data_table_df.shape[ 0 ] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at distribution of a key variable\n",
    "key_variable = \"awesomeness\"\n",
    "data_table_df[ key_variable ].value_counts( normalize = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training and testing data\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Before we can fit a model, we need to split our data into separate sets of training and testing data.\n",
    "\n",
    "For simple models where you have a pool of data you want to use to both test and train, scikit-learn provides an automated method for randomly splitting data from a single table of records into either a training or testing set.\n",
    "\n",
    "For more complex models where you have specific data you want to use to train and test (training on older data, then testing against newer data to test performance over time, for instance), you can also set up your training and testing data manually.  Examples of each are below.\n",
    "\n",
    "For the rest of the code in this notebook to run correctly, whichever method you choose, when you are done, you need the following variables set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrames to hold training and testing data, with features (X-variables)\n",
    "#     and label (y-variable) commingled.\n",
    "df_training = None\n",
    "df_testing = None\n",
    "\n",
    "# DataFrames to hold features (X-variables) and label (y-variable)\n",
    "#     for training and testing sets of data.  Should just include\n",
    "#     features and labels, not additional columns.\n",
    "X_train = None\n",
    "y_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "\n",
    "# numpy array of label/y values, for use in scikit-learn training.\n",
    "y_train_values = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify features and labels\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "To start, we specify the names of the columns that we will use as features (predictors, or X variables) and as the label (predicted, or y variable).  Make sure to set these variables no matter how you are setting up your training and testing data, as they are referenced later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make list of the names of columns that contain features in our data table.\n",
    "feature_column_names = []\n",
    "feature_column_names.append( 'feature1' )\n",
    "feature_column_names.append( 'feature2' )\n",
    "feature_column_names.append( 'feature3' )\n",
    "# ... etc.\n",
    "\n",
    "# And, capture name of label column.\n",
    "label_column_name = 'label1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing sets using scikit-learn\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "First, starting with a single DataFrame that contains all of our data (`data_table_df`), we look at using scikit-learn to randomly split a single data set into test and train sets.\n",
    "\n",
    "To start, create DataFrames that just contain features (predictors, or x-variables) and our label (the value we want to predict, or the y-variable).\n",
    "\n",
    "Split into separate Feature and Label DataFrames, where any columns not named in either `feature_column_names` or `label_column_name` are ommitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split into separate Feature and Label DataFrames\n",
    "\n",
    "# features, based on feature_column_names...\n",
    "feature_df = pandas.DataFrame.copy( data_table_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ...and the label, based on label_column_name.\n",
    "label_column_name_list = [ label_column_name ]\n",
    "label_df = data_table_df[ label_column_name_list ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split our features (predictors, or x-variables) and our label values (the value we want to predict, or the y-variable) into training and testing sets.  We'll use the `scikit-learn` `train_test_split()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# configuration\n",
    "percent_in_test = 0.25\n",
    "desired_random_state = 0\n",
    "\n",
    "# use `train_test_split` from scikit-learn.\n",
    "X_train, X_test, y_train, y_test = train_test_split( feature_df, \n",
    "                                                     label_df,\n",
    "                                                     test_size = percent_in_test,\n",
    "                                                     random_state = desired_random_state )\n",
    "\n",
    "# Filter X_train and X_test to just the features we want.\n",
    "df_testing = pandas.DataFrame.copy( X_test )\n",
    "df_training = pandas.DataFrame.copy( X_train )\n",
    "X_test = X_test[ feature_column_names ]\n",
    "X_train = X_train[ feature_column_names ]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_train_values = y_train[ label_column_name ].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL - free up memory\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "In case you need or want to, here is how you free up memory now that you have your features and labels filtered.  If you are going to be working with different sets of features or different labels, you probably don't want to do this, because it will remove your data table and feature and label data frames from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First set variables that refer to DataFrames to None.\n",
    "data_table_df = None\n",
    "feature_df = None\n",
    "label_df = None\n",
    "\n",
    "# then tell Python to collect garbage.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing sets manually\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "If you have a purposive set of training and testing data you'd like to use, the code below shows how you can set all the variables the rest of this notebook needs manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set df_testing and df_training if necessary.\n",
    "df_training = my_training_data_frame\n",
    "df_testing = my_testing_data_frame\n",
    "\n",
    "# ...and the label, based on label_column_name.\n",
    "label_column_name_list = [ label_column_name ]\n",
    "\n",
    "# create pandas series of training and testing features (X) and label (y)\n",
    "X_train = df_training[ feature_column_names ]\n",
    "y_train = df_training[ label_column_name_list ]\n",
    "X_test = df_testing[ feature_column_names ]\n",
    "y_test = df_testing[ label_column_name_list ]\n",
    "\n",
    "# Convert to numpy arrays as needed.\n",
    "\n",
    "# y_train\n",
    "y_train_values = y_train[ label_column_name ].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's fit a model\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression( penalty = 'l1', C = 1e5 )\n",
    "\n",
    "# use y_train_values - it wants a numpy array.\n",
    "model.fit( X_train, y_train_values )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Understanding\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Look at the coefficients for each of the features in the model (an indication of the weight the machine learning algrithm assigned to each feature, similar to regression beta-weights/coefficients):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"The coefficients for each of the features are:\" \n",
    "zip( feature_column_names, model.coef_[ 0 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"The standardized coefficients for each of the features are:\" \n",
    "std_coef = np.std( X_test, 0 ) * model.coef_[ 0 ]\n",
    "zip( feature_column_names, std_coef )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted vs. Expected\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score between 0 and 1 (that can sometimes be interpreted as a probability), which lets you more finely rank all of the examples from *most likely* to *least likely* to have label 1 (positive). This score is then turned into a 0 or 1 based on a user-specified threshold. For example, you might label all examples that have a score greater than 0.5 (1/2) as positive (1), but there's no reason that has to be the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  from our \"predictors\" using the model.\n",
    "y_scores = model.predict_proba( X_test )[ :,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of scores and see if it makes sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our distribution of scores is skewed, with the majority of scores on the lower end of the scale. We expect this because 79% of the training data is made up of people not returning to benefits, so we'd guess that a higher proportion of the examples in the test set will be negative (meaning they should have lower scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add set of y scores to testing data, alongside actual data.\n",
    "df_testing['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display actual values alongside y scores.\n",
    "df_testing[ [ label_column_name, 'y_score' ] ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools like `sklearn` often have a default threshold of 0.5, but a good threshold is selected based on the data, model and the specific problem you are solving. As a trial run, let's set a threshold of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "calc_threshold = lambda x, y : 0 if x < y else 1 \n",
    "predicted = np.array( [ calc_threshold( score, 0.45 ) for score in y_scores ] )\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix( expected, predicted )\n",
    "print conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score( expected, predicted )\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of interpreting accuracy score:\n",
    "\n",
    "We get an accuracy score of XX%. Recall that our testing dataset had XX% people staying off benefits and XX% off benefits. If we had just labeled all the examples as negative and guessed going back to benefits every time, we would have had an accuracy of XX%, so our basic model is not doing much better than a \"dumb classifier.\" That's ok, because we're just getting started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "Precision and recall are other ways you can look at the relationships between true and false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score( expected, predicted )\n",
    "recall = recall_score( expected, predicted )\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str( recall ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall at k percent\n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve we can focus on more fine-grained metrics. For instance, say there is a special program for people likely to need assistance within the next year , but only *3000 or 1% of the people in our test set*  can be admitted. In that case, we would want to prioritize the 1% who were *most likely* to need assistance within the next year, and it wouldn't matter too much how accurate we were on the 78% or so who weren't very likely to need assistane.\n",
    "\n",
    "Let's say that, out of the approximately 300,000 peoiple, we can intervene on 1% of them, or the \"top\" 3000 people in a year (where \"top\" means highest likelihood of needing assistance in the next year). We can then focus on optimizing our **precision at 1%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clfs = {'RF': RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "       'ET': ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='entropy'),\n",
    "        'LR': LogisticRegression(penalty='l1', C=1e5),\n",
    "        'SGD':SGDClassifier(loss='log'),\n",
    "        'GB': GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, random_state=17, n_estimators=10),\n",
    "        'NB': GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sel_clfs = ['RF', 'ET', 'LR', 'SGD', 'GB', 'NB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k = 0\n",
    "for clfNM in sel_clfs:\n",
    "    clf = clfs[clfNM]\n",
    "    clf.fit( X_train, y_train_values )\n",
    "    print clf\n",
    "    y_score = clf.predict_proba(X_test)[:,1]\n",
    "    predicted = np.array(y_score)\n",
    "    expected = np.array(y_test)\n",
    "    plot_precision_recall_n(expected,predicted, clfNM)\n",
    "    p_at_1 = precision_at_k(expected,y_score, 0.01)\n",
    "    if max_p_at_k < p_at_1:\n",
    "        max_p_at_k = p_at_1\n",
    "    print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline \n",
    "\n",
    "- Back to [Table of contents](#Table-of-contents)\n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 78% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone will not need benefits in the next year, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_p_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make a set of random scores that is the same length as y_test\n",
    "random_score = []\n",
    "for i in range( 0, len( y_test ) ):\n",
    "    random_score.append( random.uniform( 0,1 ) )\n",
    "\n",
    "# calculate predicted values\n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "\n",
    "print( \"Count of items in y_test (type = \" + str( type( y_test ) ) + \") = \" + str( len( y_test ) ) )\n",
    "print( \"Random score length: \" + str( len( random_score ) ) )\n",
    "print( \"Random predicted length: \" + str( len( random_predicted ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calcualte precision at 0.5 for random\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.01)\n",
    "print( \"Precision with random values at 0.5 precision: \" + str( random_p_at_5 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another good practice is checking against an \"expert\" or rule of thumb baseline. For example, say that talking to people at the IDHS, you find that they think it's much more likely that someone who has been on assistance multiple times already will need assistance in the future. Then you should check that your classifier does better than just labeling everyone who has had multiple past admits as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reenter_predicted = np.array([ 1 if n_spells > 3 else 0 for n_spells in df_testing.n_spells.values ])\n",
    "reenter_p_at_1 = precision_at_k(expected,reenter_predicted,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_non_reenter = np.array([0 for n_spells in df_testing.n_spells.values])\n",
    "all_non_reenter_p_at_1 = precision_at_k(expected, all_non_reenter,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"poster\", font_scale=2.25, rc={\"lines.linewidth\":2.25, \"lines.markersize\":8})\n",
    "fig, ax = plt.subplots(1, figsize=(22,12))\n",
    "sns.barplot(['Random','All no need', 'More than 3 Spell','Model'],\n",
    "            [random_p_at_5, all_non_reenter_p_at_1, reenter_p_at_1, max_p_at_k],\n",
    "            palette=['#6F777D','#6F777D','#6F777D','#800000'])\n",
    "sns.despine()\n",
    "plt.ylim(0,1)\n",
    "plt.ylabel('precision at 1%')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
